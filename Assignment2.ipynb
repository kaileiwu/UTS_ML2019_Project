{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mNdVNMQNfPTZ"
      },
      "source": [
        "# Assignment 2: Practical Machine Learning Project Report\n",
        "\n",
        "\n",
        "\n",
        "31005/32513 Machine Learning Spring 2019, Assignment 2\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "> Group members: Pit Wegner, Xianfeng Zhuge, Kailei Wu\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxgoxD4Y78rx",
        "colab_type": "text"
      },
      "source": [
        "Video Pitch: https://www.youtube.com/watch?v=ceS2Spw9g-Y\n",
        "\n",
        "Github: https://github.com/pitwegner/UTS_ML2019_Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHZc_T3yf-wX"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PQEvsEuOg4Zr"
      },
      "source": [
        "Nowadays, more and more activity recognition-based machine learning has been studied, especially in human physical activity recognition. The progress in this direction is surprisingly good. Models can recognize simple human exercises, such as walking, running or standing, count steps and detect a general overarching activity. However, it is hard to apply in a specific domain such as healthcare. There are mainly for two reasons. The activities themselves are more complex, potentially involving other subjects, and additionally harder to analyze because of the lack of public data that can be used within the research. In this report, a machine learning (ML) model regarding the \"Open Lab Nursing Activity Recognition Challenge\" is created, which asks to recognize six different activities within the nursing area (Lago et al., 2019).\n",
        "\n",
        "In recent years, many activity recognition algorithms in health care applications are focusing on patients or doctors rather than nurses. However, the recognition of nursing activities can be helpful in the nursing domain, such as for automatic record creation and standardization of operation supervision. Until now, there is still a gap for ML models to recognize nursing-related activities due to the problems that we mentioned before. The applications of such a model would be wide-spread and of high impact in the nursing area, given the model achieves an exceptional accuracy in prediction.\n",
        "\n",
        "In this report, a machine learning technique to tackle the \"Open Lab Nursing Activity Recognition Challenge\" is proposed and evaluated. First, the data provided will be explored, including data preprocessing and data structure design. Next, the CNN model implementation that challenged the problem will be presented, followed by a test result performance analysis of the model and an evaluation for the whole experiment. Before concluding, methods of improvement and future work regarding the project are formulated. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bFX72tlphNQ9"
      },
      "source": [
        "## Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vI6pLj7bhSGO"
      },
      "source": [
        "### Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bhqrFrDJo09I"
      },
      "source": [
        "For training purposes, the “Open Lab Nursing Activity Recognition Challenge” provides three sets of data from different sensors (acceleration, motion capture, meditag), labelled with 6 different nursing activities performed by 6 persons. In the experiment, only the motion capture data is used, comprised of 29 positional sensors. The dataset is split into multiple labelled segments of 60 seconds each. For each segment, the sensor positions (x, y, z coordinates) have been recorded, resulting in 87 data points and a timestamp. The activity label is inferred from a join of the segment id with the label dataset, which also includes the nurse id (Lago et al., 2019)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I-SE87VGo2lc"
      },
      "source": [
        "### Data preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f8f0WbPao5fq"
      },
      "source": [
        "The data source for the project can be configured to be Google Drive, GitHub or the downloaded local repository. The official data source is only accessible via a regularly expiring link, inhibiting a direct download. After the download, each input file is read and saved in dataframes. To clean the dataset from NaN values, the techniques of front-filling, back-filling and finally zero-filling are used. The order is important here. Front-filling fills up missing values with the previous one, which makes sense for time series data. The back-filling afterwards fills up empty rows before the capture of the first value, padding the start. If the dataframe still includes NaN values afterwards, it must mean that there is an entire sensor missing for that segment. Then the column is filled with zeros. Since the time intervals are constant for all samples, it is also safe to remove the timestamp from the dataset and maintain ordering by the index. In order to prevent exploding and vanishing gradients, a min-max normalization step is applied before defining the final dataset. When accessing the dataset for training, the segment id is dropped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Ob8N6-U2Y6T",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import pandas as pd\n",
        "import glob\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "COLAB = True\n",
        "DATA_SOURCE = 'github' # can be google, github, or local\n",
        "LOCATION_PREFIX = '' # path of data (or download location for github)\n",
        "\n",
        "\n",
        "colab_root = '/content/' if COLAB else ''\n",
        "\n",
        "if DATA_SOURCE == 'google':\n",
        "    from google.colab import drive\n",
        "    drive.mount(colab_root + 'drive')\n",
        "    LOCATION_PREFIX = colab_root + 'drive/My Drive/' + LOCATION_PREFIX\n",
        "elif DATA_SOURCE == 'github':\n",
        "    import urllib.request\n",
        "    \n",
        "    dl_location = colab_root + LOCATION_PREFIX\n",
        "    print(\"Downloading Data...\")\n",
        "    filename, headers = urllib.request.urlretrieve('https://github.com/pitwegner/UTS_ML2019_Project/archive/master.zip', filename=dl_location + 'master.zip')\n",
        "    import zipfile\n",
        "    print(\"Extracting...\")\n",
        "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(dl_location)\n",
        "    LOCATION_PREFIX = dl_location + 'UTS_ML2019_Project-master/data/'\n",
        "    \n",
        "print(\"Reading Labels...\")\n",
        "# Read activity labels for segments and nurse id\n",
        "activities = pd.read_csv(LOCATION_PREFIX + \"activities_train.csv\")\n",
        "activity_arr = activities.activity_id.unique()\n",
        "activity_arr.sort()\n",
        "\n",
        "# Read Motion Capture Data\n",
        "mocap = pd.DataFrame()\n",
        "print(\"Reading Mocap Data...\")\n",
        "i = 0\n",
        "bar_length = 50\n",
        "files = glob.glob(LOCATION_PREFIX + \"mocap/segment*.csv\")\n",
        "for mf in files:\n",
        "    # Basic NaN value removal\n",
        "    mocap = mocap.append(pd.read_csv(mf).ffill().bfill().fillna(0))\n",
        "    i += 1\n",
        "    progress = math.ceil(bar_length * i / len(files))\n",
        "    print(\"\\r\", \"[\" + \"=\" * progress + \" \" * (bar_length - progress) + \"] \" + \"{0:.2f}\".format(100 * i / len(files)) + '%', end=\"\")\n",
        "\n",
        "print(\"\\nCreating Index...\")\n",
        "# Drop time column, since constant frequency\n",
        "mocap = mocap.reset_index().drop(columns=['index','time_elapsed'])\n",
        "\n",
        "# Min-max normalization\n",
        "print(\"Normalizing...\")\n",
        "mocap_normalized = (mocap-mocap.min())/(mocap.max()-mocap.min())\n",
        "mocap_normalized.segment_id = mocap.segment_id\n",
        "mocap = mocap_normalized\n",
        "print(\"Done.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BjXRr0w72Y6g",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  \n",
        "    def __init__(self, train, labels):\n",
        "        self.labels = labels\n",
        "        self.data = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.data[index].drop(columns=['segment_id']).values\n",
        "        sid = self.data[index].segment_id.unique()[0]\n",
        "        labels = self.labels[self.labels.segment_id == sid]\n",
        "        aid = labels.activity_id.values[0]\n",
        "        y = np.array([activity_arr.tolist().index(aid), sid])\n",
        "\n",
        "        return X, y\n",
        "\n",
        "dataset = Dataset(mocap, activities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F9u2BIbe2Y6o"
      },
      "source": [
        "A data inspection shows the distribution of persons and activity segments in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XVHDRgKc2Y6s",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "print(\"Person\", \":\", \"[(activity_id, segment count), ...]\")\n",
        "for a in activities.subject.unique():\n",
        "    act = activities[activities.subject == a]\n",
        "    print(a, \":\", [(i, len(act[act.activity_id == i].segment_id)) for i in np.sort(act.activity_id.unique())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5VFsFB6-o7Gf"
      },
      "source": [
        "### Data Split and Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "reXEhFVQo86H"
      },
      "source": [
        "After the data preprocessing, two different approaches for the data split are provided: Splitting the data  randomly or splitting the data by  person. For the person split, four persons are chosen for training, one person for validation and one for testing. This results in an approximate split of 70-15-15. For a random split, the segments are shuffled and also split by the same percentages. Afterwards, the window indices are determined by the window length of 200 (twice the frequency) and the stride of 50 for each segment. By saving only the indices, memory can be saved, not  storing  redundant  information. For each resulting dataset, a RandomWindowSampler handles the access during training, selecting the window of the fixed window length from a random permutation  of the indices. This way, each window is selected exactly once per epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvyqOEj_2Y62",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import Sampler    \n",
        "\n",
        "window_length = 200 # = 2*f\n",
        "\n",
        "# Sampler that iterates a random permutation of start indices and selects window\n",
        "class RandomWindowSampler(Sampler):\n",
        "  \n",
        "    def __init__(self, indices):\n",
        "        self.indices = indices\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (slice(self.indices[i], self.indices[i] + window_length) for i in torch.randperm(len(self.indices)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Yi84cas2Y7B",
        "colab": {}
      },
      "source": [
        "# Split data either randomly by segments or by person (as official test set introduces a new person)\n",
        "PERSON_SPLIT = True\n",
        "TEST_PERSON = 2\n",
        "VAL_PERSON = 4\n",
        "\n",
        "if PERSON_SPLIT:\n",
        "    indices = {}\n",
        "    for sid in dataset.data.segment_id.unique():\n",
        "        i = list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
        "        p = activities[activities.segment_id == sid].subject.item()\n",
        "        if p not in indices:\n",
        "            indices[p] = []\n",
        "        indices[p] += i\n",
        "\n",
        "    test_indices = indices.pop(TEST_PERSON)\n",
        "    val_indices = indices.pop(VAL_PERSON)\n",
        "    train_indices = [item for sublist in indices.values() for item in sublist]\n",
        "else:\n",
        "    segments = dataset.data.segment_id.unique()\n",
        "    np.random.shuffle(segments)\n",
        "    \n",
        "    split = int(np.floor(0.15 * len(segments))) # 70% training, 15% validation, 15% testing\n",
        "    train, val, test = segments[split+split:], segments[split:split+split], segments[:split]\n",
        "    \n",
        "    train_indices, val_indices, test_indices = ([],[],[])\n",
        "    for sid in train:\n",
        "        train_indices += list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
        "    for sid in val:\n",
        "        val_indices += list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
        "    for sid in test:\n",
        "        test_indices += list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
        "\n",
        "train_sampler = RandomWindowSampler(train_indices)\n",
        "val_sampler = RandomWindowSampler(val_indices)\n",
        "test_sampler = RandomWindowSampler(test_indices)\n",
        "\n",
        "# Create data loaders to parallelize batch training to multiple cores\n",
        "def get_train_loader(batch_size):\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=3)\n",
        "val_loader = torch.utils.data.DataLoader(dataset, batch_size=128, sampler=val_sampler, num_workers=3)\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=4, sampler=test_sampler, num_workers=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X8L7fSIqo-xP"
      },
      "source": [
        "### Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "90jvCtr6pBHn"
      },
      "source": [
        "The model architecture is a Convolutional Neural Network (CNN) including a convolutional layer, a pooling layer and two fully connected layers. The RELU activation function is used on both the convolution layer and the first fully connected layer. The loss function we used for training and validation loss is cross-entropy loss, a standard for multi-class classification with probability outputs between 0 an 1. The torch implementation also applies the softmax activation function to the output, before calculating the loss. As an optimizer, adaptive moment estimation (ADAM) was applied. The architectural idea is based on the work of Bevilacqua et al. (2018). The code is largely inspired by a pytorch CNN tutorial (Algorithmia, 2019).\n",
        "\n",
        "\n",
        "![Architecture](https://raw.githubusercontent.com/pitwegner/UTS_ML2019_Project/master/img/architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gYA-nO4z2Y7N",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        kernel_size = 3\n",
        "        stride = 1\n",
        "        padding = 1\n",
        "        output_channels = 24\n",
        "        \n",
        "        pooling_size = 2\n",
        "        pooling_stride = 2\n",
        "        pooling_padding = 0\n",
        "        hidden_parameters = 64\n",
        "        \n",
        "        # Calculate output size after convolution\n",
        "        conv_output_x = int((window_length - kernel_size + 2 * padding) / stride) + 1\n",
        "        conv_output_y = int((dataset[0:1][0].shape[1] - kernel_size + 2 * padding) / stride) + 1\n",
        "        pool_output_x = int((conv_output_x - pooling_size + 2 * pooling_padding) / pooling_stride) + 1\n",
        "        pool_output_y = int((conv_output_y - pooling_size + 2 * pooling_padding) / pooling_stride) + 1\n",
        "        self.dense_input = output_channels * pool_output_x * pool_output_y\n",
        "        \n",
        "        self.conv1 = torch.nn.Conv2d(1, output_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=pooling_size, stride=pooling_stride, padding=pooling_padding)\n",
        "        self.fc1 = torch.nn.Linear(self.dense_input, hidden_parameters)\n",
        "        self.fc2 = torch.nn.Linear(hidden_parameters, len(activity_arr))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, self.dense_input)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IFdyE-wZpCw5"
      },
      "source": [
        "### Test Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EbAYv5pHpEg9"
      },
      "source": [
        "As an activity recognition model, the accuracy of the model is measured by its classification  performance. The challenge requires to predict the activity of a given segment. In  order  to  test the model accuracy, the activity for each window is predicted and compared to the label  data. The final test aggregates the predicitions for each segment, selecting the mode as a final decision (majority vote). The  results are  compared to the activity labels to compute the overall accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "blsJmMd72Y7b",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "def testNet(net):\n",
        "    confusion_matrix = np.zeros((6,6))\n",
        "    votes = {}\n",
        "    print(\"Starting Test Run\")\n",
        "\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.reshape((inputs.shape[0], 1, inputs.shape[1], inputs.shape[2]))\n",
        "        inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        val_outputs = net(inputs)\n",
        "        predictions = val_outputs.argmax(1)\n",
        "\n",
        "        # Collect votes\n",
        "        for s in range(len(labels)):\n",
        "            segment = labels[s,1].item()\n",
        "            if segment not in votes:\n",
        "                votes[segment] = []\n",
        "            votes[segment].append(predictions[s].item())\n",
        "\n",
        "        # Compare test vs. label\n",
        "        confusion_matrix[predictions, labels[:,0]] += 1\n",
        "        print(\"\\r\", \"{0:.2f}%\".format(100 * i / len(test_loader)), end=\"\")\n",
        "\n",
        "    # Select prediction as vote majority\n",
        "    correct_votes = 0\n",
        "    for sid in votes:\n",
        "        label = dataset.labels[dataset.labels.segment_id == sid].activity_id.values[0]\n",
        "        votes[sid] = [activity_arr[stats.mode(votes[sid])[0][0]], label]\n",
        "        correct_votes += int(votes[sid][0] == votes[sid][1])\n",
        "\n",
        "    print(\"\\rConfusion matrix for individual windows:\")\n",
        "    print(confusion_matrix)\n",
        "    print(\"Accuracy: {0:.2f}%\".format(100 * np.trace(confusion_matrix)/np.sum(confusion_matrix)))\n",
        "    print(\"Vote prediction for entire segments:\")\n",
        "    print(votes)\n",
        "    print(\"Accuracy: {0:.2f}%\".format(100 * correct_votes/len(votes)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rxdg99fYhS07"
      },
      "source": [
        "## Methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lb7XUqFAL8Vn"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mzViw9WAL8z0"
      },
      "source": [
        "For training the network, a batch size of 32 was selected, resulting in about 500 loops per epoch. Each loop includes a forward pass, backward pass and optimisation step, updating the weights. Training is performed with a learning rate of 0.000001. Comparisons showed unstable learning or no learning at all for larger learning rates. Decreasing the learning rate resulted in a smooth learning curve, making it easier to decide when to stop training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "foyYYjXKoqbr"
      },
      "source": [
        "### Model Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oRKcINedhTGl"
      },
      "source": [
        "At the end of each epoch, a validation pass on the validation set is performed to measure an independent loss during training. The validation result cannot be used for training, but is only an observation to recognize overfitting the training data. Thus, it serves as a measure for when to stop training. As a heuristic, training is stopped when the validation loss has not decreased for 10 epochs. Each time the validation loss improves, the model at that state is saved for future testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z30blYyK6NaN",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
        "    print(\"Training started\")\n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_batches = len(train_loader)\n",
        "    \n",
        "    # Select loss function and optimizer\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    min_val_loss = math.inf\n",
        "    worse_counter = 0\n",
        "    \n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        total_train_loss = 0.0\n",
        "        print_every = 25\n",
        "        start_time = time.time()\n",
        "        \n",
        "        worse_counter += 1\n",
        "        \n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            # reshape data since we only have 1 channel\n",
        "            inputs = inputs.reshape((inputs.shape[0], 1, inputs.shape[1], inputs.shape[2]))\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            # Forward pass, backward pass, optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss_size = loss(outputs, labels[:,0])\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Aggregate losses for plotting and printing\n",
        "            running_loss += loss_size.data.item()\n",
        "            total_train_loss += loss_size.data.item()\n",
        "            \n",
        "            # Print average running loss every 25th batch of an epoch\n",
        "            if (i + 1) % print_every == 0:\n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(epoch + 1, int(100 * (i + 1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "        \n",
        "        # Run validation pass at end of epoch\n",
        "        total_val_loss = 0.0\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.reshape((inputs.shape[0], 1, inputs.shape[1], inputs.shape[2]))\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "            val_outputs = net(inputs)\n",
        "            val_loss_size = loss(val_outputs, labels[:,0])\n",
        "            total_val_loss += val_loss_size.data.item()\n",
        "            \n",
        "        loss_avg = total_val_loss / len(val_loader)\n",
        "        print(\"Validation loss = {:.2f}{}\".format(loss_avg, ' (worse, {})'.format(worse_counter) if loss_avg >= min_val_loss else ' (better)'))\n",
        "        if loss_avg < min_val_loss:\n",
        "            min_val_loss = loss_avg\n",
        "            worse_counter = 0\n",
        "            \n",
        "            # Save best model for testing\n",
        "            best_model = SimpleCNN().load_state_dict(net.state_dict())\n",
        "        \n",
        "        # Append average loss for plotting\n",
        "        val_losses.append(loss_avg)\n",
        "        train_losses.append(total_train_loss / n_batches)\n",
        "        \n",
        "        # Stop training if we haven't improved for 10 epochs\n",
        "        if worse_counter >= 10:\n",
        "            break\n",
        "        \n",
        "    print(\"Training finished, took {}s\".format(time.time() - training_start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7a1bApf6fXf",
        "colab": {}
      },
      "source": [
        "CNN = SimpleCNN()\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_model = CNN\n",
        "\n",
        "# Train run\n",
        "trainNet(CNN.double(), batch_size=32, n_epochs=150, learning_rate=0.000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO7wloKx_j8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss visualization\n",
        "import plotly.graph_objects as go\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=np.arange(len(train_losses)), y=train_losses, mode='lines', name='train_loss'))\n",
        "fig.add_trace(go.Scatter(x=np.arange(len(val_losses)), y=val_losses, mode='lines', name='val_loss'))\n",
        "fig.show()\n",
        "\n",
        "# Test run\n",
        "testNet(best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoYFvtUn78tu",
        "colab_type": "text"
      },
      "source": [
        "### Alternative Approaches\n",
        "\n",
        "The traditional way to tackle time series classification is using Recurrent Neural Networks (RNN). A comparison between the two approaches in terms of accuracy and efficiency could yield insight into the relatively new approach of using CNN to detect patterns in time series. Also within the capabilities of CNN, there are more transformations to try for this kind of data. For example, X, Y and Z could be interpreted as channels, similar to pictures. This could make more use of the pattern-recognition abilities of convolutional layers. The challenge summary (Lago et al., 2019) also shows different approaches, such as KNN and random forests, which would be interesting to compare against this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tONIZV0ohTS3"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F_tOUGIxhTaL"
      },
      "source": [
        "In this project, the data can be split in two different ways. In a random split method, the data is shuffled and randomly sampled into training, validation and test set. This, however, is not realistic. The model needs to learn the abstract activity, independent of the performing subject. This also corresponds to the challenge, which evaluated all participants based on a test set of 2 new subjects. Hence in the person split method, the data is split according to the performing person. This way, there are four persons in the training set, one person in the validation set and one person in the test set. Both methods will be evaluated in this chapter.\n",
        "\n",
        "### Model Performance\n",
        "\n",
        "#### Random Split\n",
        "\n",
        "The random split method results quickly in relatively high accuracy results. The figure below indicates the loss of both validation and training set after 75 epochs of training. The training loss steadily decreases, as does the validation loss. However, the validation loss fluctuates, indicating that further learning should be possible with more epochs.\n",
        "\n",
        "![Random_Segment_Split](https://raw.githubusercontent.com/pitwegner/UTS_ML2019_Project/master/img/plot_random_segment_split.png)\n",
        "\n",
        "The table below represents the random split confusion matrix of window prediction, where each column represents the real label of each window and the row represents the prediction of each window. From the table, we observe that about half of the predictions made were accurate, with the most uncertainty for activity 2, 6 and 9. Aggragating the predictions for each segment using the majority vote, the accuracy improves to 71.43%.\n",
        "\n",
        "Confusion matrix of window prediction with random split (52.71% accuracy):\n",
        "\n",
        "|     *    | $l_2$ | $l_3$ | $l_4$ | $l_6$ | $l_9$ | $l_{12}$ |\n",
        "| -------- | ----- | ----- | ----- | ----- | ----- | -------- |\n",
        "| $p_2$    |   366 |   103 |    68 |   185 |    61 |       39 |\n",
        "| $p_3$    |    88 |   411 |    36 |     4 |     5 |        3 |\n",
        "| $p_4$    |    83 |    80 |   181 |    49 |   210 |       62 |\n",
        "| $p_6$    |    25 |    25 |     2 |   146 |    74 |       92 |\n",
        "| $p_9$    |    39 |     4 |     6 |    32 |   269 |      103 |\n",
        "| $p_{12}$ |   171 |    10 |    21 |    27 |   142 |      688 |\n",
        "\n",
        "Segment prediction using majority vote (71.43% accuracy):\n",
        "\n",
        "| segment | 4 | 31 | 56 | 57 | 63 | 88 | 101 | 173 | 185 | 194 | 215 | 227 | 241 | 248 | 290 | 295 | 305 | 306 | 336 | 362 | 373 |\n",
        "| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
        "| label | 2 | 4 | 12 | 2 | 2 | 2 | 6 | 9 | 9 | 12 | 3 | 9 | 2 | 3 | 4 | 12 | 3 | 12 | 9 | 3 | 9 |\n",
        "| predict | 2 | 2 | 12 | 2 | 4 | 2 | 2 | 9 | 9 | 12 | 2 | 12 | 2 | 3 | 4 | 12 | 3 | 12 | 6 | 3 | 9 |\n",
        "| **segment** | **385** | **480** | **508** | **515** | **531** | **559** | **573** | **607** | **620** | **631** | **639** | **640** | **667** | **689** | **703** | **711** | **748** | **753** | **765** | **777** | **795** |\n",
        "| label | 3 | 2 | 6 | 9 | 6 | 12 | 3 | 6 | 3 | 12 | 12 | 12 | 12 | 2 | 9 | 12 | 2 | 4 | 9 | 12 | 12 |\n",
        "| predict | 3 | 12 | 4 | 12 | 2 | 12 | 3 | 6 | 4 | 12 | 12 | 12 | 12 | 2 | 4 | 12 | 2 | 4 | 9 | 12 | 12 |\n",
        "\n",
        "#### Person Split\n",
        "\n",
        "Contrary to the random split method, the person split method learns much slower, but more stable. The training stopped after ~80 epochs at a validation loss of about 1.25, compared to the still running random split method at a validation loss of about 1.15. This is expected, since the abstraction is harder to learn and more susceptible to overfitting. The result produced an accuracy of 31.04% on single windows. More than half of Activities 2,3,4,9 and 6 are predicted wrong, while only activity 12 has correctly predicted the majority. The reason behind this might be the distinct motion of activity 12, which is 'Indwelling drip retention and connection'. Especially the prediction of activity 6 fails almost for every window. \n",
        "\n",
        "![Person_Split](https://raw.githubusercontent.com/pitwegner/UTS_ML2019_Project/master/img/plot_person_split.png)\n",
        "\n",
        "Confusion matrix of window prediction with person split (31.04% accuracy):\n",
        "\n",
        "|     *    | $l_2$ | $l_3$ | $l_4$ | $l_6$ | $l_9$ | $l_{12}$ |\n",
        "| -------- | ----- | ----- | ----- | ----- | ----- | -------- |\n",
        "| $p_2$    |   186 |   212 |   205 |   231 |    78 |      148 |\n",
        "| $p_3$    |    60 |   305 |    34 |     0 |     3 |        0 |\n",
        "| $p_4$    |    46 |    58 |   126 |    17 |    81 |      118 |\n",
        "| $p_6$    |     7 |    18 |     0 |    45 |    33 |       49 |\n",
        "| $p_9$    |    51 |   103 |   104 |    13 |   207 |      118 |\n",
        "| $p_{12}$ |   371 |   437 |    78 |   122 |   138 |      451 |\n",
        "\n",
        "Segment prediction using majority vote (44.19% accuracy):\n",
        "\n",
        "| segment | 0 | 56 | 67 | 85 | 99 | 138 | 143 | 144 | 145 | 189 | 196 | 200 | 210 | 213 | 219 | 227 | 229 | 243 | 283 | 305 | 328 |\n",
        "| - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - | - |\n",
        "| label | 6 | 12 | 12 | 2 | 3 | 3 | 3 | 4 | 12 | 6 | 2 | 9 | 9 | 3 | 2 | 9 | 2 | 9 | 4 | 3 | 3 |\n",
        "| predict | 2 | 12 | 12 | 2 | 3 | 12 | 2 | 2 | 12 | 2 | 12 | 12 | 12 | 12 | 12 | 9 | 2 | 9 | 2 | 2 | 12 |\n",
        "| **segment** | **340** | **380** | **387** | **406** | **432** | **441** | **459** | **515** | **604** | **612** | **613** | **625** | **639** | **662** | **669** | **674** | **680** | **682** | **685** | **686** | **723** | **773** |\n",
        "| label | 12 | 9 | 12 | 4 | 12 | 12 | 3 | 9 | 12 | 3 | 2 | 4 | 12 | 3 | 4 | 3 | 6 | 2 | 2 | 12 | 6 | 3 |\n",
        "| predict | 12 | 2 | 12 | 2 | 12 | 12 | 12 | 9 | 12 | 2 | 2 | 9 | 12 | 3 | 4 | 3 | 2 | 12 | 12 | 4 | 2 | 12 |\n",
        "\n",
        "Due to the high prediction rate of activity 12 in window prediction, activity 12 got the highest perdiction amount other activities in segment prediction. This leads to an interesting insight that 90% of activity 12 is predicted correctly, while the accuracy of other activities are very low. And also, due to low accuracy of the window prediction of activity 6, the segment prediction accuracy of activity 6 is down to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7_135wN0hTe8"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EJHpJvsvhTli"
      },
      "source": [
        "Using CNN for the classification of spatial time series for activity recognition has proven to work to some extent in the complex healthcare domain. For the explained use cases of automatic documentation, however, this prototype is not sufficient. The tuning of learning rate is just one example of many, how the model can be further improved, next to architectural changes in depth and type. Majority voting was shown to be efficient for determining a prediction of a larger context based on smaller steps. This experimental project also lead to the insight that overall prediction results may be overshadowed by one single exceptionally good candidate.\n",
        "\n",
        "Comparing the random split method to the person split, the accuracy is much better for random trainig due to slight personal differences in execution of the same activity. However, the 44.19% accuracy on an unseen person ranks at about the baseline of the challenge. As the labels for the test dataset are not available, a true comparison is unfortunately not possible.\n",
        "\n",
        "### Future Work\n",
        "\n",
        "There are three kinds of data provided in the paper, which are motion capture, accelerometer and meditag data. However, in our model, we only use the motion capture data. Therefore, one logical improvement is data augumentation, which could protect from overfitting and increase accuracy by introducing more potentially important features. Also, since we only used the raw data after norminalization, other feature extraction methods could be applied before training.  \n",
        "\n",
        "Secondly, some changes could be made about the network architecture. Instead of only using one convolution layer in CNN network, a deeper network could be applied to detect higher-level features, potentially combined with more data input. Furthermore, more traditional deep learning techniques can be used in the model, such as learning decay, Xavier & He initialization, and batch normalization. Also, since the the data is the sequence data, which is the domain of RNN network, a combination of a CNN feature extractor and RNN sequence classifier could yield interesting results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jtcC0IahmoHv"
      },
      "source": [
        "## Ethical Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6OqpS59Emva8"
      },
      "source": [
        "As a machine learning model, common ethical issues considering the application in a real world domain arise naturally. These typically aim at potential mistakes, training bias and data protection (Osoba & Welser, 2017). Escpecially in the medical domain, mistakes can have a huge impact on a human life. In the case of our model, however, a wrong documentation entry about a nurse activity has a limited capability to directly cause damage. Patient health can be compromised by a left out or wrong procedure, for example developing pressure ulcers as a result of improper turning and repositioning of bedridden patients. The documentation record can then be used to trace back where the mistake has happened. Thus, our model provides support in preventing and retracing clinical incidents without directly impacting patient safety. Additionally, giving the nurse a chance to manually adjust the report after its generation further mitigates the impact of erroneous results. Other benefits of applying the model in nursing include a decreased workload and a more complete activity report for accounting purposes.\n",
        "\n",
        "Activity recognition in a workplace also adds ethical complications. If not protected, tool could easily be (mis-)used by managers to monitor workforce activity, intruding into their privacy. The insights could result in different treatment for individual employees, even up to terminating the employment. Privacy impairment from the data pool itself is a further issue, since detailed movements are possibly visible from motion capture data, even if much harder to process than, say video. Thus, secure storage and access to data, as well as a proper authorization to the model results are crucial for an implementation.\n",
        "\n",
        "Considering the results being biased toward training data, the risk of discrimination only leads to more inaccurate results for new employees. As this is known and can be tested beforehand, the impact is minimal. Overall, with proper data protection and sufficient accuracy, the benefits of the technique outweigh the possible risks. From a utalitarian point of view, the use of our model can hence be considered ethically acceptable. A deontological approach also does not object. Since results can be modified, the autonomy and freedom of will are not inhibited by applying the model, as long as privacy is ensured. Furthermore, the universalization of the application neither contradict with the intention nor with itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lteotOwi78t8",
        "colab_type": "text"
      },
      "source": [
        "## Reference List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9R07Ijz78uA",
        "colab_type": "text"
      },
      "source": [
        "Algorithmia 2019, ‘Convolutional Neural Nets in PyTorch’, *Algorithmia Blog*, weblog, 10 April, viewed 25 September 2019, https://blog.algorithmia.com/convolutional-neural-nets-in-pytorch. \n",
        "\n",
        "Bevilacqua, A., MacDonald, K., Rangarej, A., Widjaya, V., Caulfield, B. & Kechadi, T. 2018, 'Human Activity Recognition with Convolutional Neural Networks', in *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*, Springer, Cham, pp. 541-552.\n",
        "\n",
        "Lago, P., Alia, S. S., Takeda, S., Mairittha, T., Mairittha, N., Faiz, F., Nishimura, Y., Adachi, K., Okita, T., Charpillet, F. & Inoue, S. 2019. 'Nurse care activity recognition challenge: summary and results', in Proceedings of the *2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing* and *Proceedings of the 2019 ACM International Symposium on Wearable Computers*, ACM, pp. 746-751.\n",
        "\n",
        "Lago, P., Alia, S.S., Shamma, A., Mairittha, T., Mairittha, N. & Inoue, Z. 2019. 'Open Lab Nursing Activity Recognition Challenge', *IEEE Data Port*.\n",
        "\n",
        "Osoba, O.A. & Welser IV, W. 2017, 'An intelligence in our image: The risks of bias and errors in artificial intelligence', Rand Corporation, Santa Monica, Calif."
      ]
    }
  ]
}