{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "A2_Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzn2si-aysUL",
        "colab_type": "text"
      },
      "source": [
        "## Notes\n",
        "\n",
        "* All segments have different lengths -> sliding window approach\n",
        "  * maybe length of 100 or 200? I read that the window size matters a lot\n",
        "\n",
        "## Open Questions\n",
        "\n",
        "* Network architecture? -> LSTM, CNN, Hierarchical Attention?\n",
        "  * Maybe implement multiple and compare?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReiUL--rysUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-RL7JyBysUi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9334bf6c-a982-4500-b177-e329395fcb0f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "activities = pd.read_csv(\"/content/drive/My Drive/train/activities_train.csv\") # Activity Labels for Segments and Nurse ID\n",
        "mocap = pd.DataFrame()\n",
        "print(\"Reading Mocap Data\")\n",
        "i = 0\n",
        "bar_length = 50\n",
        "files = glob.glob(\"/content/drive/My Drive/train/mocap/segment*.csv\")\n",
        "for mf in files:\n",
        "    i += 1\n",
        "    progress = math.ceil(bar_length * i / len(files))\n",
        "    print(\"\\r\", \"[\" + \"=\" * progress + \" \" * (bar_length - progress) + \"] \" + \"{0:.2f}\".format(100 * i / len(files)) + '%', end=\"\")\n",
        "    mocap = mocap.append(pd.read_csv(mf).ffill().bfill().fillna(0))\n",
        "mocap = mocap.reset_index().drop(columns=['index','time_elapsed'])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Reading Mocap Data\n",
            " [==================================================] 100.00%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM-losmrzvIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mocap_normalized = (mocap-mocap.min())/(mocap.max()-mocap.min())\n",
        "mocap_normalized.segment_id = mocap.segment_id\n",
        "mocap = mocap_normalized\n",
        "activity_arr = activities.activity_id.unique()\n",
        "activity_arr.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8N1M0M0W2_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  \n",
        "  def __init__(self, train, labels):\n",
        "        self.labels = labels\n",
        "        self.data = train\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        X = self.data[index].drop(columns=['segment_id']).values\n",
        "        sid = self.data[index].segment_id.unique()[0]\n",
        "        labels = self.labels[self.labels.segment_id == sid]\n",
        "        aid = labels.activity_id.values[0]\n",
        "        y = activity_arr.tolist().index(aid)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "dataset = Dataset(mocap, activities)\n",
        "window_length = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-qq2hNXysVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleCNN(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        self.kernel_size = 3\n",
        "        self.stride = 1\n",
        "        self.padding = 1\n",
        "        self.output_channels = 24\n",
        "        self.hidden_parameters = 64\n",
        "        \n",
        "        self.output_x = int((window_length - self.kernel_size + 2 * self.padding) / self.stride) + 1\n",
        "        self.output_y = int((dataset[0:1][0].shape[1] - self.kernel_size + 2 * self.padding) / self.stride) + 1\n",
        "        \n",
        "        self.conv1 = torch.nn.Conv2d(1, self.output_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = torch.nn.Linear(self.output_channels * int(self.output_x / 2) * int(self.output_y / 2), self.hidden_parameters)\n",
        "        self.fc2 = torch.nn.Linear(self.hidden_parameters, len(activities.activity_id.unique()))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, self.output_channels * int(self.output_x / 2) * int(self.output_y / 2))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9yWA08P7wFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import Sampler    \n",
        "\n",
        "class RandomWindowSampler(Sampler):\n",
        "  \n",
        "  def __init__(self, indices):\n",
        "    self.indices = indices\n",
        "  \n",
        "  def __iter__(self):\n",
        "    return (slice(self.indices[i], self.indices[i] + window_length) for i in torch.randperm(len(self.indices)))\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.indices)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdZrZemGHkB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = []\n",
        "for sid in dataset.data.segment_id.unique():\n",
        "  indices += list(dataset.data[dataset.data.segment_id == sid].index[:-window_length])\n",
        "\n",
        "split = int(np.floor(0.2 * len(indices)))\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = RandomWindowSampler(train_indices)\n",
        "val_sampler = RandomWindowSampler(val_indices)\n",
        "\n",
        "def get_train_loader(batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
        "    return(train_loader)\n",
        "  \n",
        "val_loader = torch.utils.data.DataLoader(dataset, batch_size=128, sampler=val_sampler, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB02w7U8IJK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    \n",
        "    #Loss function\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    #Optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return(loss, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZvSGvD2I5JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
        "  \n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size =\", batch_size)\n",
        "    print(\"epochs =\", n_epochs)\n",
        "    print(\"learning_rate =\", learning_rate)\n",
        "    print(\"=\" * 27)\n",
        "    \n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_batches = len(train_loader)\n",
        "    \n",
        "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    \n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        print_every = 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs.reshape((32,1,200,87))), Variable(labels)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #Forward pass, backward pass, optimize\n",
        "            outputs = net(inputs)\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            #Print statistics\n",
        "            running_loss += loss_size.data.item()\n",
        "            total_train_loss += loss_size.data.item()\n",
        "            \n",
        "            #Print every 10th batch of an epoch\n",
        "            if (i + 1) % (print_every) == 0:\n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                        epoch+1, int(100 * (i+1) / 300), running_loss / print_every, time.time() - start_time))\n",
        "                #Reset running loss and time\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "                \n",
        "            if (i + 1) >= 300:\n",
        "                break\n",
        "            \n",
        "        #At the end of the epoch, do a pass on the validation set\n",
        "        total_val_loss = 0\n",
        "        for i, data in enumerate(val_loader, 0):\n",
        "            \n",
        "            #Wrap tensors in Variables\n",
        "            inputs, labels = data\n",
        "            inputs, labels = Variable(inputs.reshape((128,1,200,87))), Variable(labels)\n",
        "            #Forward pass\n",
        "            val_outputs = net(inputs)\n",
        "            val_loss_size = loss(val_outputs, labels)\n",
        "            total_val_loss += val_loss_size.data.item()\n",
        "            if (i + 1) >= 100:\n",
        "              break\n",
        "            \n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / 100))\n",
        "        \n",
        "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCwSnqOBJerf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a759ea1-531a-4c43-e4b2-20f2d8d7f523"
      },
      "source": [
        "CNN = SimpleCNN()\n",
        "trainNet(CNN.double(), batch_size=32, n_epochs=20, learning_rate=0.001)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size = 32\n",
            "epochs = 20\n",
            "learning_rate = 0.001\n",
            "===========================\n",
            "Epoch 1, 3% \t train_loss: 6.52 took: 6.65s\n",
            "Epoch 1, 6% \t train_loss: 2.14 took: 5.80s\n",
            "Epoch 1, 10% \t train_loss: 1.85 took: 5.82s\n",
            "Epoch 1, 13% \t train_loss: 1.75 took: 5.82s\n",
            "Epoch 1, 16% \t train_loss: 1.63 took: 5.82s\n",
            "Epoch 1, 20% \t train_loss: 1.49 took: 5.84s\n",
            "Epoch 1, 23% \t train_loss: 1.47 took: 5.81s\n",
            "Epoch 1, 26% \t train_loss: 1.41 took: 5.84s\n",
            "Epoch 1, 30% \t train_loss: 1.43 took: 5.83s\n",
            "Epoch 1, 33% \t train_loss: 1.33 took: 5.83s\n",
            "Epoch 1, 36% \t train_loss: 1.41 took: 5.83s\n",
            "Epoch 1, 40% \t train_loss: 1.44 took: 5.81s\n",
            "Epoch 1, 43% \t train_loss: 1.31 took: 5.86s\n",
            "Epoch 1, 46% \t train_loss: 1.44 took: 5.81s\n",
            "Epoch 1, 50% \t train_loss: 1.37 took: 5.86s\n",
            "Epoch 1, 53% \t train_loss: 1.26 took: 5.80s\n",
            "Epoch 1, 56% \t train_loss: 1.23 took: 5.81s\n",
            "Epoch 1, 60% \t train_loss: 1.34 took: 5.80s\n",
            "Epoch 1, 63% \t train_loss: 1.27 took: 5.85s\n",
            "Epoch 1, 66% \t train_loss: 1.38 took: 5.76s\n",
            "Epoch 1, 70% \t train_loss: 1.29 took: 5.83s\n",
            "Epoch 1, 73% \t train_loss: 1.31 took: 5.82s\n",
            "Epoch 1, 76% \t train_loss: 1.38 took: 5.85s\n",
            "Epoch 1, 80% \t train_loss: 1.20 took: 5.84s\n",
            "Epoch 1, 83% \t train_loss: 1.27 took: 5.76s\n",
            "Epoch 1, 86% \t train_loss: 1.14 took: 5.83s\n",
            "Epoch 1, 90% \t train_loss: 1.18 took: 5.81s\n",
            "Epoch 1, 93% \t train_loss: 1.24 took: 5.84s\n",
            "Epoch 1, 96% \t train_loss: 1.22 took: 5.86s\n",
            "Epoch 1, 100% \t train_loss: 1.14 took: 5.81s\n",
            "Validation loss = 1.16\n",
            "Epoch 2, 3% \t train_loss: 1.12 took: 6.77s\n",
            "Epoch 2, 6% \t train_loss: 1.12 took: 5.90s\n",
            "Epoch 2, 10% \t train_loss: 1.04 took: 5.94s\n",
            "Epoch 2, 13% \t train_loss: 1.23 took: 5.86s\n",
            "Epoch 2, 16% \t train_loss: 1.07 took: 5.91s\n",
            "Epoch 2, 20% \t train_loss: 1.19 took: 5.91s\n",
            "Epoch 2, 23% \t train_loss: 1.21 took: 5.90s\n",
            "Epoch 2, 26% \t train_loss: 1.14 took: 5.91s\n",
            "Epoch 2, 30% \t train_loss: 1.13 took: 5.87s\n",
            "Epoch 2, 33% \t train_loss: 1.16 took: 5.85s\n",
            "Epoch 2, 36% \t train_loss: 1.20 took: 5.88s\n",
            "Epoch 2, 40% \t train_loss: 1.10 took: 5.88s\n",
            "Epoch 2, 43% \t train_loss: 1.16 took: 5.88s\n",
            "Epoch 2, 46% \t train_loss: 1.13 took: 5.87s\n",
            "Epoch 2, 50% \t train_loss: 1.04 took: 5.92s\n",
            "Epoch 2, 53% \t train_loss: 1.05 took: 5.83s\n",
            "Epoch 2, 56% \t train_loss: 1.11 took: 5.90s\n",
            "Epoch 2, 60% \t train_loss: 1.11 took: 5.85s\n",
            "Epoch 2, 63% \t train_loss: 1.03 took: 5.86s\n",
            "Epoch 2, 66% \t train_loss: 1.02 took: 5.82s\n",
            "Epoch 2, 70% \t train_loss: 1.04 took: 5.86s\n",
            "Epoch 2, 73% \t train_loss: 0.99 took: 5.83s\n",
            "Epoch 2, 76% \t train_loss: 0.99 took: 5.83s\n",
            "Epoch 2, 80% \t train_loss: 1.02 took: 5.84s\n",
            "Epoch 2, 83% \t train_loss: 0.97 took: 5.84s\n",
            "Epoch 2, 86% \t train_loss: 1.04 took: 5.85s\n",
            "Epoch 2, 90% \t train_loss: 1.00 took: 5.85s\n",
            "Epoch 2, 93% \t train_loss: 1.12 took: 5.84s\n",
            "Epoch 2, 96% \t train_loss: 1.02 took: 5.84s\n",
            "Epoch 2, 100% \t train_loss: 1.03 took: 5.82s\n",
            "Validation loss = 0.93\n",
            "Epoch 3, 3% \t train_loss: 0.89 took: 6.71s\n",
            "Epoch 3, 6% \t train_loss: 0.98 took: 5.91s\n",
            "Epoch 3, 10% \t train_loss: 0.93 took: 5.77s\n",
            "Epoch 3, 13% \t train_loss: 0.95 took: 5.82s\n",
            "Epoch 3, 16% \t train_loss: 0.83 took: 5.82s\n",
            "Epoch 3, 20% \t train_loss: 0.89 took: 5.86s\n",
            "Epoch 3, 23% \t train_loss: 0.91 took: 5.87s\n",
            "Epoch 3, 26% \t train_loss: 0.85 took: 5.84s\n",
            "Epoch 3, 30% \t train_loss: 0.93 took: 5.82s\n",
            "Epoch 3, 33% \t train_loss: 0.97 took: 5.85s\n",
            "Epoch 3, 36% \t train_loss: 0.94 took: 5.82s\n",
            "Epoch 3, 40% \t train_loss: 0.94 took: 5.82s\n",
            "Epoch 3, 43% \t train_loss: 0.86 took: 5.81s\n",
            "Epoch 3, 46% \t train_loss: 0.87 took: 5.87s\n",
            "Epoch 3, 50% \t train_loss: 0.80 took: 5.83s\n",
            "Epoch 3, 53% \t train_loss: 0.87 took: 5.82s\n",
            "Epoch 3, 56% \t train_loss: 0.85 took: 5.84s\n",
            "Epoch 3, 60% \t train_loss: 0.96 took: 5.85s\n",
            "Epoch 3, 63% \t train_loss: 0.88 took: 5.84s\n",
            "Epoch 3, 66% \t train_loss: 0.86 took: 5.80s\n",
            "Epoch 3, 70% \t train_loss: 0.90 took: 5.79s\n",
            "Epoch 3, 73% \t train_loss: 0.87 took: 5.83s\n",
            "Epoch 3, 76% \t train_loss: 0.81 took: 5.82s\n",
            "Epoch 3, 80% \t train_loss: 0.78 took: 5.75s\n",
            "Epoch 3, 83% \t train_loss: 0.71 took: 5.79s\n",
            "Epoch 3, 86% \t train_loss: 0.78 took: 5.77s\n",
            "Epoch 3, 90% \t train_loss: 0.86 took: 5.75s\n",
            "Epoch 3, 93% \t train_loss: 0.77 took: 5.81s\n",
            "Epoch 3, 96% \t train_loss: 0.83 took: 5.79s\n",
            "Epoch 3, 100% \t train_loss: 0.84 took: 5.82s\n",
            "Validation loss = 0.81\n",
            "Epoch 4, 3% \t train_loss: 0.77 took: 6.63s\n",
            "Epoch 4, 6% \t train_loss: 0.79 took: 5.80s\n",
            "Epoch 4, 10% \t train_loss: 0.72 took: 5.83s\n",
            "Epoch 4, 13% \t train_loss: 0.72 took: 5.81s\n",
            "Epoch 4, 16% \t train_loss: 0.76 took: 5.84s\n",
            "Epoch 4, 20% \t train_loss: 0.72 took: 5.80s\n",
            "Epoch 4, 23% \t train_loss: 0.68 took: 5.83s\n",
            "Epoch 4, 26% \t train_loss: 0.65 took: 5.79s\n",
            "Epoch 4, 30% \t train_loss: 0.77 took: 5.81s\n",
            "Epoch 4, 33% \t train_loss: 0.80 took: 5.83s\n",
            "Epoch 4, 36% \t train_loss: 0.85 took: 5.80s\n",
            "Epoch 4, 40% \t train_loss: 0.72 took: 5.80s\n",
            "Epoch 4, 43% \t train_loss: 0.72 took: 5.79s\n",
            "Epoch 4, 46% \t train_loss: 0.78 took: 5.80s\n",
            "Epoch 4, 50% \t train_loss: 0.73 took: 5.82s\n",
            "Epoch 4, 53% \t train_loss: 0.75 took: 5.80s\n",
            "Epoch 4, 56% \t train_loss: 0.72 took: 5.85s\n",
            "Epoch 4, 60% \t train_loss: 0.81 took: 5.84s\n",
            "Epoch 4, 63% \t train_loss: 0.70 took: 5.81s\n",
            "Epoch 4, 66% \t train_loss: 0.75 took: 5.76s\n",
            "Epoch 4, 70% \t train_loss: 0.62 took: 5.78s\n",
            "Epoch 4, 73% \t train_loss: 0.69 took: 5.82s\n",
            "Epoch 4, 76% \t train_loss: 0.69 took: 5.79s\n",
            "Epoch 4, 80% \t train_loss: 0.64 took: 5.77s\n",
            "Epoch 4, 83% \t train_loss: 0.71 took: 5.80s\n",
            "Epoch 4, 86% \t train_loss: 0.69 took: 5.81s\n",
            "Epoch 4, 90% \t train_loss: 0.63 took: 5.86s\n",
            "Epoch 4, 93% \t train_loss: 0.66 took: 5.81s\n",
            "Epoch 4, 96% \t train_loss: 0.63 took: 5.85s\n",
            "Epoch 4, 100% \t train_loss: 0.66 took: 5.76s\n",
            "Validation loss = 0.65\n",
            "Epoch 5, 3% \t train_loss: 0.67 took: 6.71s\n",
            "Epoch 5, 6% \t train_loss: 0.62 took: 5.74s\n",
            "Epoch 5, 10% \t train_loss: 0.70 took: 5.77s\n",
            "Epoch 5, 13% \t train_loss: 0.61 took: 5.84s\n",
            "Epoch 5, 16% \t train_loss: 0.62 took: 5.83s\n",
            "Epoch 5, 20% \t train_loss: 0.67 took: 5.82s\n",
            "Epoch 5, 23% \t train_loss: 0.64 took: 5.77s\n",
            "Epoch 5, 26% \t train_loss: 0.63 took: 5.84s\n",
            "Epoch 5, 30% \t train_loss: 0.65 took: 5.87s\n",
            "Epoch 5, 33% \t train_loss: 0.75 took: 5.79s\n",
            "Epoch 5, 36% \t train_loss: 0.70 took: 5.83s\n",
            "Epoch 5, 40% \t train_loss: 0.77 took: 5.82s\n",
            "Epoch 5, 43% \t train_loss: 0.64 took: 5.83s\n",
            "Epoch 5, 46% \t train_loss: 0.73 took: 5.81s\n",
            "Epoch 5, 50% \t train_loss: 0.59 took: 5.82s\n",
            "Epoch 5, 53% \t train_loss: 0.72 took: 5.79s\n",
            "Epoch 5, 56% \t train_loss: 0.60 took: 5.84s\n",
            "Epoch 5, 60% \t train_loss: 0.64 took: 5.82s\n",
            "Epoch 5, 63% \t train_loss: 0.61 took: 5.86s\n",
            "Epoch 5, 66% \t train_loss: 0.57 took: 5.82s\n",
            "Epoch 5, 70% \t train_loss: 0.64 took: 5.84s\n",
            "Epoch 5, 73% \t train_loss: 0.70 took: 5.87s\n",
            "Epoch 5, 76% \t train_loss: 0.69 took: 5.82s\n",
            "Epoch 5, 80% \t train_loss: 0.60 took: 5.83s\n",
            "Epoch 5, 83% \t train_loss: 0.63 took: 5.82s\n",
            "Epoch 5, 86% \t train_loss: 0.59 took: 5.86s\n",
            "Epoch 5, 90% \t train_loss: 0.66 took: 5.80s\n",
            "Epoch 5, 93% \t train_loss: 0.57 took: 5.81s\n",
            "Epoch 5, 96% \t train_loss: 0.56 took: 5.84s\n",
            "Epoch 5, 100% \t train_loss: 0.60 took: 5.79s\n",
            "Validation loss = 0.58\n",
            "Epoch 6, 3% \t train_loss: 0.55 took: 6.67s\n",
            "Epoch 6, 6% \t train_loss: 0.49 took: 5.76s\n",
            "Epoch 6, 10% \t train_loss: 0.59 took: 5.81s\n",
            "Epoch 6, 13% \t train_loss: 0.48 took: 5.75s\n",
            "Epoch 6, 16% \t train_loss: 0.57 took: 5.83s\n",
            "Epoch 6, 20% \t train_loss: 0.59 took: 5.80s\n",
            "Epoch 6, 23% \t train_loss: 0.65 took: 5.78s\n",
            "Epoch 6, 26% \t train_loss: 0.57 took: 5.79s\n",
            "Epoch 6, 30% \t train_loss: 0.59 took: 5.75s\n",
            "Epoch 6, 33% \t train_loss: 0.57 took: 5.73s\n",
            "Epoch 6, 36% \t train_loss: 0.52 took: 5.78s\n",
            "Epoch 6, 40% \t train_loss: 0.55 took: 5.83s\n",
            "Epoch 6, 43% \t train_loss: 0.55 took: 5.79s\n",
            "Epoch 6, 46% \t train_loss: 0.62 took: 5.77s\n",
            "Epoch 6, 50% \t train_loss: 0.65 took: 5.90s\n",
            "Epoch 6, 53% \t train_loss: 0.60 took: 5.83s\n",
            "Epoch 6, 56% \t train_loss: 0.68 took: 5.75s\n",
            "Epoch 6, 60% \t train_loss: 0.57 took: 5.73s\n",
            "Epoch 6, 63% \t train_loss: 0.55 took: 5.79s\n",
            "Epoch 6, 66% \t train_loss: 0.63 took: 5.77s\n",
            "Epoch 6, 70% \t train_loss: 0.60 took: 5.82s\n",
            "Epoch 6, 73% \t train_loss: 0.56 took: 5.77s\n",
            "Epoch 6, 76% \t train_loss: 0.56 took: 5.80s\n",
            "Epoch 6, 80% \t train_loss: 0.56 took: 5.76s\n",
            "Epoch 6, 83% \t train_loss: 0.56 took: 5.81s\n",
            "Epoch 6, 86% \t train_loss: 0.56 took: 5.83s\n",
            "Epoch 6, 90% \t train_loss: 0.53 took: 5.87s\n",
            "Epoch 6, 93% \t train_loss: 0.52 took: 5.80s\n",
            "Epoch 6, 96% \t train_loss: 0.56 took: 5.79s\n",
            "Epoch 6, 100% \t train_loss: 0.55 took: 5.82s\n",
            "Validation loss = 0.49\n",
            "Epoch 7, 3% \t train_loss: 0.53 took: 6.69s\n",
            "Epoch 7, 6% \t train_loss: 0.56 took: 5.82s\n",
            "Epoch 7, 10% \t train_loss: 0.54 took: 5.81s\n",
            "Epoch 7, 13% \t train_loss: 0.47 took: 5.90s\n",
            "Epoch 7, 16% \t train_loss: 0.51 took: 5.84s\n",
            "Epoch 7, 20% \t train_loss: 0.56 took: 5.80s\n",
            "Epoch 7, 23% \t train_loss: 0.48 took: 5.79s\n",
            "Epoch 7, 26% \t train_loss: 0.56 took: 5.79s\n",
            "Epoch 7, 30% \t train_loss: 0.60 took: 5.86s\n",
            "Epoch 7, 33% \t train_loss: 0.56 took: 5.76s\n",
            "Epoch 7, 36% \t train_loss: 0.52 took: 5.82s\n",
            "Epoch 7, 40% \t train_loss: 0.51 took: 5.78s\n",
            "Epoch 7, 43% \t train_loss: 0.57 took: 5.81s\n",
            "Epoch 7, 46% \t train_loss: 0.46 took: 5.79s\n",
            "Epoch 7, 50% \t train_loss: 0.43 took: 5.82s\n",
            "Epoch 7, 53% \t train_loss: 0.50 took: 5.79s\n",
            "Epoch 7, 56% \t train_loss: 0.50 took: 5.83s\n",
            "Epoch 7, 60% \t train_loss: 0.52 took: 5.77s\n",
            "Epoch 7, 63% \t train_loss: 0.53 took: 5.81s\n",
            "Epoch 7, 66% \t train_loss: 0.57 took: 5.75s\n",
            "Epoch 7, 70% \t train_loss: 0.56 took: 5.74s\n",
            "Epoch 7, 73% \t train_loss: 0.63 took: 5.75s\n",
            "Epoch 7, 76% \t train_loss: 0.55 took: 5.77s\n",
            "Epoch 7, 80% \t train_loss: 0.44 took: 5.85s\n",
            "Epoch 7, 83% \t train_loss: 0.50 took: 5.77s\n",
            "Epoch 7, 86% \t train_loss: 0.54 took: 5.78s\n",
            "Epoch 7, 90% \t train_loss: 0.48 took: 5.77s\n",
            "Epoch 7, 93% \t train_loss: 0.52 took: 5.77s\n",
            "Epoch 7, 96% \t train_loss: 0.54 took: 5.78s\n",
            "Epoch 7, 100% \t train_loss: 0.53 took: 5.77s\n",
            "Validation loss = 0.53\n",
            "Epoch 8, 3% \t train_loss: 0.49 took: 6.66s\n",
            "Epoch 8, 6% \t train_loss: 0.55 took: 5.79s\n",
            "Epoch 8, 10% \t train_loss: 0.49 took: 5.82s\n",
            "Epoch 8, 13% \t train_loss: 0.53 took: 5.72s\n",
            "Epoch 8, 16% \t train_loss: 0.56 took: 5.81s\n",
            "Epoch 8, 20% \t train_loss: 0.46 took: 5.75s\n",
            "Epoch 8, 23% \t train_loss: 0.48 took: 5.80s\n",
            "Epoch 8, 26% \t train_loss: 0.49 took: 5.80s\n",
            "Epoch 8, 30% \t train_loss: 0.49 took: 5.78s\n",
            "Epoch 8, 33% \t train_loss: 0.43 took: 5.75s\n",
            "Epoch 8, 36% \t train_loss: 0.43 took: 5.88s\n",
            "Epoch 8, 40% \t train_loss: 0.53 took: 5.81s\n",
            "Epoch 8, 43% \t train_loss: 0.51 took: 5.84s\n",
            "Epoch 8, 46% \t train_loss: 0.48 took: 5.80s\n",
            "Epoch 8, 50% \t train_loss: 0.55 took: 5.79s\n",
            "Epoch 8, 53% \t train_loss: 0.51 took: 5.78s\n",
            "Epoch 8, 56% \t train_loss: 0.43 took: 5.81s\n",
            "Epoch 8, 60% \t train_loss: 0.44 took: 5.79s\n",
            "Epoch 8, 63% \t train_loss: 0.48 took: 5.75s\n",
            "Epoch 8, 66% \t train_loss: 0.47 took: 5.78s\n",
            "Epoch 8, 70% \t train_loss: 0.54 took: 5.84s\n",
            "Epoch 8, 73% \t train_loss: 0.50 took: 5.81s\n",
            "Epoch 8, 76% \t train_loss: 0.48 took: 5.82s\n",
            "Epoch 8, 80% \t train_loss: 0.49 took: 5.77s\n",
            "Epoch 8, 83% \t train_loss: 0.48 took: 5.83s\n",
            "Epoch 8, 86% \t train_loss: 0.41 took: 5.84s\n",
            "Epoch 8, 90% \t train_loss: 0.47 took: 5.83s\n",
            "Epoch 8, 93% \t train_loss: 0.52 took: 5.81s\n",
            "Epoch 8, 96% \t train_loss: 0.43 took: 5.83s\n",
            "Epoch 8, 100% \t train_loss: 0.44 took: 5.83s\n",
            "Validation loss = 0.48\n",
            "Epoch 9, 3% \t train_loss: 0.45 took: 6.67s\n",
            "Epoch 9, 6% \t train_loss: 0.43 took: 5.78s\n",
            "Epoch 9, 10% \t train_loss: 0.42 took: 5.82s\n",
            "Epoch 9, 13% \t train_loss: 0.47 took: 5.80s\n",
            "Epoch 9, 16% \t train_loss: 0.45 took: 5.81s\n",
            "Epoch 9, 20% \t train_loss: 0.45 took: 5.83s\n",
            "Epoch 9, 23% \t train_loss: 0.43 took: 5.83s\n",
            "Epoch 9, 26% \t train_loss: 0.51 took: 5.78s\n",
            "Epoch 9, 30% \t train_loss: 0.51 took: 5.85s\n",
            "Epoch 9, 33% \t train_loss: 0.45 took: 5.83s\n",
            "Epoch 9, 36% \t train_loss: 0.45 took: 5.86s\n",
            "Epoch 9, 40% \t train_loss: 0.45 took: 5.82s\n",
            "Epoch 9, 43% \t train_loss: 0.50 took: 5.89s\n",
            "Epoch 9, 46% \t train_loss: 0.50 took: 5.81s\n",
            "Epoch 9, 50% \t train_loss: 0.40 took: 5.89s\n",
            "Epoch 9, 53% \t train_loss: 0.45 took: 5.82s\n",
            "Epoch 9, 56% \t train_loss: 0.45 took: 5.83s\n",
            "Epoch 9, 60% \t train_loss: 0.40 took: 5.79s\n",
            "Epoch 9, 63% \t train_loss: 0.44 took: 5.90s\n",
            "Epoch 9, 66% \t train_loss: 0.43 took: 5.85s\n",
            "Epoch 9, 70% \t train_loss: 0.47 took: 5.83s\n",
            "Epoch 9, 73% \t train_loss: 0.35 took: 5.85s\n",
            "Epoch 9, 76% \t train_loss: 0.36 took: 5.77s\n",
            "Epoch 9, 80% \t train_loss: 0.36 took: 5.84s\n",
            "Epoch 9, 83% \t train_loss: 0.50 took: 5.81s\n",
            "Epoch 9, 86% \t train_loss: 0.47 took: 5.82s\n",
            "Epoch 9, 90% \t train_loss: 0.57 took: 5.80s\n",
            "Epoch 9, 93% \t train_loss: 0.49 took: 5.76s\n",
            "Epoch 9, 96% \t train_loss: 0.50 took: 5.79s\n",
            "Epoch 9, 100% \t train_loss: 0.48 took: 5.80s\n",
            "Validation loss = 0.42\n",
            "Epoch 10, 3% \t train_loss: 0.43 took: 6.68s\n",
            "Epoch 10, 6% \t train_loss: 0.42 took: 5.83s\n",
            "Epoch 10, 10% \t train_loss: 0.41 took: 5.77s\n",
            "Epoch 10, 13% \t train_loss: 0.46 took: 5.81s\n",
            "Epoch 10, 16% \t train_loss: 0.42 took: 5.79s\n",
            "Epoch 10, 20% \t train_loss: 0.40 took: 5.83s\n",
            "Epoch 10, 23% \t train_loss: 0.39 took: 5.84s\n",
            "Epoch 10, 26% \t train_loss: 0.35 took: 5.82s\n",
            "Epoch 10, 30% \t train_loss: 0.42 took: 5.93s\n",
            "Epoch 10, 33% \t train_loss: 0.52 took: 5.80s\n",
            "Epoch 10, 36% \t train_loss: 0.49 took: 5.88s\n",
            "Epoch 10, 40% \t train_loss: 0.37 took: 5.86s\n",
            "Epoch 10, 43% \t train_loss: 0.36 took: 5.87s\n",
            "Epoch 10, 46% \t train_loss: 0.42 took: 5.84s\n",
            "Epoch 10, 50% \t train_loss: 0.43 took: 5.86s\n",
            "Epoch 10, 53% \t train_loss: 0.38 took: 5.90s\n",
            "Epoch 10, 56% \t train_loss: 0.40 took: 5.82s\n",
            "Epoch 10, 60% \t train_loss: 0.39 took: 5.84s\n",
            "Epoch 10, 63% \t train_loss: 0.36 took: 5.80s\n",
            "Epoch 10, 66% \t train_loss: 0.44 took: 5.81s\n",
            "Epoch 10, 70% \t train_loss: 0.41 took: 5.84s\n",
            "Epoch 10, 73% \t train_loss: 0.41 took: 5.85s\n",
            "Epoch 10, 76% \t train_loss: 0.40 took: 5.80s\n",
            "Epoch 10, 80% \t train_loss: 0.35 took: 5.77s\n",
            "Epoch 10, 83% \t train_loss: 0.47 took: 5.86s\n",
            "Epoch 10, 86% \t train_loss: 0.35 took: 5.79s\n",
            "Epoch 10, 90% \t train_loss: 0.41 took: 5.80s\n",
            "Epoch 10, 93% \t train_loss: 0.39 took: 5.78s\n",
            "Epoch 10, 96% \t train_loss: 0.46 took: 5.87s\n",
            "Epoch 10, 100% \t train_loss: 0.40 took: 5.79s\n",
            "Validation loss = 0.40\n",
            "Epoch 11, 3% \t train_loss: 0.44 took: 6.65s\n",
            "Epoch 11, 6% \t train_loss: 0.41 took: 5.77s\n",
            "Epoch 11, 10% \t train_loss: 0.41 took: 5.79s\n",
            "Epoch 11, 13% \t train_loss: 0.39 took: 5.77s\n",
            "Epoch 11, 16% \t train_loss: 0.48 took: 5.72s\n",
            "Epoch 11, 20% \t train_loss: 0.45 took: 5.70s\n",
            "Epoch 11, 23% \t train_loss: 0.39 took: 5.76s\n",
            "Epoch 11, 26% \t train_loss: 0.49 took: 5.77s\n",
            "Epoch 11, 30% \t train_loss: 0.39 took: 5.70s\n",
            "Epoch 11, 33% \t train_loss: 0.41 took: 5.74s\n",
            "Epoch 11, 36% \t train_loss: 0.32 took: 5.79s\n",
            "Epoch 11, 40% \t train_loss: 0.37 took: 5.77s\n",
            "Epoch 11, 43% \t train_loss: 0.49 took: 5.72s\n",
            "Epoch 11, 46% \t train_loss: 0.39 took: 5.77s\n",
            "Epoch 11, 50% \t train_loss: 0.42 took: 5.77s\n",
            "Epoch 11, 53% \t train_loss: 0.34 took: 5.74s\n",
            "Epoch 11, 56% \t train_loss: 0.28 took: 5.70s\n",
            "Epoch 11, 60% \t train_loss: 0.37 took: 5.70s\n",
            "Epoch 11, 63% \t train_loss: 0.39 took: 5.72s\n",
            "Epoch 11, 66% \t train_loss: 0.35 took: 5.74s\n",
            "Epoch 11, 70% \t train_loss: 0.39 took: 5.86s\n",
            "Epoch 11, 73% \t train_loss: 0.38 took: 5.68s\n",
            "Epoch 11, 76% \t train_loss: 0.32 took: 5.71s\n",
            "Epoch 11, 80% \t train_loss: 0.38 took: 5.71s\n",
            "Epoch 11, 83% \t train_loss: 0.34 took: 5.77s\n",
            "Epoch 11, 86% \t train_loss: 0.40 took: 5.67s\n",
            "Epoch 11, 90% \t train_loss: 0.43 took: 5.69s\n",
            "Epoch 11, 93% \t train_loss: 0.34 took: 5.69s\n",
            "Epoch 11, 96% \t train_loss: 0.41 took: 5.77s\n",
            "Epoch 11, 100% \t train_loss: 0.46 took: 5.74s\n",
            "Validation loss = 0.34\n",
            "Epoch 12, 3% \t train_loss: 0.34 took: 6.44s\n",
            "Epoch 12, 6% \t train_loss: 0.36 took: 5.63s\n",
            "Epoch 12, 10% \t train_loss: 0.42 took: 5.58s\n",
            "Epoch 12, 13% \t train_loss: 0.36 took: 5.61s\n",
            "Epoch 12, 16% \t train_loss: 0.36 took: 5.65s\n",
            "Epoch 12, 20% \t train_loss: 0.42 took: 5.65s\n",
            "Epoch 12, 23% \t train_loss: 0.39 took: 5.70s\n",
            "Epoch 12, 26% \t train_loss: 0.41 took: 5.66s\n",
            "Epoch 12, 30% \t train_loss: 0.35 took: 5.68s\n",
            "Epoch 12, 33% \t train_loss: 0.39 took: 5.63s\n",
            "Epoch 12, 36% \t train_loss: 0.35 took: 5.71s\n",
            "Epoch 12, 40% \t train_loss: 0.36 took: 5.64s\n",
            "Epoch 12, 43% \t train_loss: 0.39 took: 5.65s\n",
            "Epoch 12, 46% \t train_loss: 0.28 took: 5.65s\n",
            "Epoch 12, 50% \t train_loss: 0.34 took: 5.63s\n",
            "Epoch 12, 53% \t train_loss: 0.32 took: 5.68s\n",
            "Epoch 12, 56% \t train_loss: 0.39 took: 5.66s\n",
            "Epoch 12, 60% \t train_loss: 0.37 took: 5.70s\n",
            "Epoch 12, 63% \t train_loss: 0.41 took: 5.68s\n",
            "Epoch 12, 66% \t train_loss: 0.36 took: 5.66s\n",
            "Epoch 12, 70% \t train_loss: 0.36 took: 5.66s\n",
            "Epoch 12, 73% \t train_loss: 0.33 took: 5.68s\n",
            "Epoch 12, 76% \t train_loss: 0.36 took: 5.68s\n",
            "Epoch 12, 80% \t train_loss: 0.41 took: 5.66s\n",
            "Epoch 12, 83% \t train_loss: 0.34 took: 5.67s\n",
            "Epoch 12, 86% \t train_loss: 0.39 took: 5.64s\n",
            "Epoch 12, 90% \t train_loss: 0.43 took: 5.63s\n",
            "Epoch 12, 93% \t train_loss: 0.33 took: 5.65s\n",
            "Epoch 12, 96% \t train_loss: 0.39 took: 5.65s\n",
            "Epoch 12, 100% \t train_loss: 0.40 took: 5.61s\n",
            "Validation loss = 0.33\n",
            "Epoch 13, 3% \t train_loss: 0.35 took: 6.43s\n",
            "Epoch 13, 6% \t train_loss: 0.37 took: 5.66s\n",
            "Epoch 13, 10% \t train_loss: 0.39 took: 5.68s\n",
            "Epoch 13, 13% \t train_loss: 0.40 took: 5.64s\n",
            "Epoch 13, 16% \t train_loss: 0.37 took: 5.66s\n",
            "Epoch 13, 20% \t train_loss: 0.36 took: 5.70s\n",
            "Epoch 13, 23% \t train_loss: 0.35 took: 5.66s\n",
            "Epoch 13, 26% \t train_loss: 0.29 took: 5.65s\n",
            "Epoch 13, 30% \t train_loss: 0.29 took: 5.64s\n",
            "Epoch 13, 33% \t train_loss: 0.41 took: 5.65s\n",
            "Epoch 13, 36% \t train_loss: 0.37 took: 5.62s\n",
            "Epoch 13, 40% \t train_loss: 0.35 took: 5.67s\n",
            "Epoch 13, 43% \t train_loss: 0.34 took: 5.69s\n",
            "Epoch 13, 46% \t train_loss: 0.33 took: 5.67s\n",
            "Epoch 13, 50% \t train_loss: 0.32 took: 5.70s\n",
            "Epoch 13, 53% \t train_loss: 0.37 took: 5.68s\n",
            "Epoch 13, 56% \t train_loss: 0.37 took: 5.71s\n",
            "Epoch 13, 60% \t train_loss: 0.34 took: 5.71s\n",
            "Epoch 13, 63% \t train_loss: 0.33 took: 5.74s\n",
            "Epoch 13, 66% \t train_loss: 0.40 took: 5.68s\n",
            "Epoch 13, 70% \t train_loss: 0.30 took: 5.73s\n",
            "Epoch 13, 73% \t train_loss: 0.33 took: 5.74s\n",
            "Epoch 13, 76% \t train_loss: 0.37 took: 5.71s\n",
            "Epoch 13, 80% \t train_loss: 0.24 took: 5.72s\n",
            "Epoch 13, 83% \t train_loss: 0.44 took: 5.70s\n",
            "Epoch 13, 86% \t train_loss: 0.34 took: 5.77s\n",
            "Epoch 13, 90% \t train_loss: 0.34 took: 5.76s\n",
            "Epoch 13, 93% \t train_loss: 0.39 took: 5.67s\n",
            "Epoch 13, 96% \t train_loss: 0.41 took: 5.63s\n",
            "Epoch 13, 100% \t train_loss: 0.41 took: 5.70s\n",
            "Validation loss = 0.42\n",
            "Epoch 14, 3% \t train_loss: 0.33 took: 6.47s\n",
            "Epoch 14, 6% \t train_loss: 0.33 took: 5.65s\n",
            "Epoch 14, 10% \t train_loss: 0.43 took: 5.69s\n",
            "Epoch 14, 13% \t train_loss: 0.41 took: 5.67s\n",
            "Epoch 14, 16% \t train_loss: 0.31 took: 5.70s\n",
            "Epoch 14, 20% \t train_loss: 0.33 took: 5.67s\n",
            "Epoch 14, 23% \t train_loss: 0.40 took: 5.66s\n",
            "Epoch 14, 26% \t train_loss: 0.39 took: 5.65s\n",
            "Epoch 14, 30% \t train_loss: 0.40 took: 5.69s\n",
            "Epoch 14, 33% \t train_loss: 0.33 took: 5.62s\n",
            "Epoch 14, 36% \t train_loss: 0.28 took: 5.64s\n",
            "Epoch 14, 40% \t train_loss: 0.30 took: 5.65s\n",
            "Epoch 14, 43% \t train_loss: 0.36 took: 5.65s\n",
            "Epoch 14, 46% \t train_loss: 0.39 took: 5.67s\n",
            "Epoch 14, 50% \t train_loss: 0.33 took: 5.72s\n",
            "Epoch 14, 53% \t train_loss: 0.38 took: 5.70s\n",
            "Epoch 14, 56% \t train_loss: 0.31 took: 5.72s\n",
            "Epoch 14, 60% \t train_loss: 0.30 took: 5.64s\n",
            "Epoch 14, 63% \t train_loss: 0.28 took: 5.73s\n",
            "Epoch 14, 66% \t train_loss: 0.34 took: 5.70s\n",
            "Epoch 14, 70% \t train_loss: 0.36 took: 5.70s\n",
            "Epoch 14, 73% \t train_loss: 0.31 took: 5.68s\n",
            "Epoch 14, 76% \t train_loss: 0.28 took: 5.74s\n",
            "Epoch 14, 80% \t train_loss: 0.36 took: 5.76s\n",
            "Epoch 14, 83% \t train_loss: 0.31 took: 5.81s\n",
            "Epoch 14, 86% \t train_loss: 0.32 took: 5.73s\n",
            "Epoch 14, 90% \t train_loss: 0.38 took: 5.69s\n",
            "Epoch 14, 93% \t train_loss: 0.30 took: 5.71s\n",
            "Epoch 14, 96% \t train_loss: 0.33 took: 5.72s\n",
            "Epoch 14, 100% \t train_loss: 0.37 took: 5.72s\n",
            "Validation loss = 0.37\n",
            "Epoch 15, 3% \t train_loss: 0.31 took: 6.54s\n",
            "Epoch 15, 6% \t train_loss: 0.35 took: 5.81s\n",
            "Epoch 15, 10% \t train_loss: 0.38 took: 5.79s\n",
            "Epoch 15, 13% \t train_loss: 0.32 took: 5.74s\n",
            "Epoch 15, 16% \t train_loss: 0.29 took: 5.79s\n",
            "Epoch 15, 20% \t train_loss: 0.40 took: 5.79s\n",
            "Epoch 15, 23% \t train_loss: 0.34 took: 5.77s\n",
            "Epoch 15, 26% \t train_loss: 0.31 took: 5.80s\n",
            "Epoch 15, 30% \t train_loss: 0.36 took: 5.78s\n",
            "Epoch 15, 33% \t train_loss: 0.28 took: 5.79s\n",
            "Epoch 15, 36% \t train_loss: 0.38 took: 5.79s\n",
            "Epoch 15, 40% \t train_loss: 0.37 took: 5.73s\n",
            "Epoch 15, 43% \t train_loss: 0.37 took: 5.75s\n",
            "Epoch 15, 46% \t train_loss: 0.31 took: 5.80s\n",
            "Epoch 15, 50% \t train_loss: 0.31 took: 5.75s\n",
            "Epoch 15, 53% \t train_loss: 0.29 took: 5.71s\n",
            "Epoch 15, 56% \t train_loss: 0.33 took: 5.72s\n",
            "Epoch 15, 60% \t train_loss: 0.31 took: 5.71s\n",
            "Epoch 15, 63% \t train_loss: 0.30 took: 5.75s\n",
            "Epoch 15, 66% \t train_loss: 0.33 took: 5.68s\n",
            "Epoch 15, 70% \t train_loss: 0.36 took: 5.70s\n",
            "Epoch 15, 73% \t train_loss: 0.30 took: 5.74s\n",
            "Epoch 15, 76% \t train_loss: 0.28 took: 5.77s\n",
            "Epoch 15, 80% \t train_loss: 0.37 took: 5.75s\n",
            "Epoch 15, 83% \t train_loss: 0.34 took: 5.70s\n",
            "Epoch 15, 86% \t train_loss: 0.33 took: 5.73s\n",
            "Epoch 15, 90% \t train_loss: 0.38 took: 5.73s\n",
            "Epoch 15, 93% \t train_loss: 0.30 took: 5.77s\n",
            "Epoch 15, 96% \t train_loss: 0.33 took: 5.72s\n",
            "Epoch 15, 100% \t train_loss: 0.24 took: 5.72s\n",
            "Validation loss = 0.32\n",
            "Epoch 16, 3% \t train_loss: 0.29 took: 6.57s\n",
            "Epoch 16, 6% \t train_loss: 0.32 took: 5.77s\n",
            "Epoch 16, 10% \t train_loss: 0.32 took: 5.81s\n",
            "Epoch 16, 13% \t train_loss: 0.31 took: 5.75s\n",
            "Epoch 16, 16% \t train_loss: 0.33 took: 5.73s\n",
            "Epoch 16, 20% \t train_loss: 0.35 took: 5.78s\n",
            "Epoch 16, 23% \t train_loss: 0.32 took: 5.78s\n",
            "Epoch 16, 26% \t train_loss: 0.29 took: 5.72s\n",
            "Epoch 16, 30% \t train_loss: 0.35 took: 5.78s\n",
            "Epoch 16, 33% \t train_loss: 0.29 took: 5.69s\n",
            "Epoch 16, 36% \t train_loss: 0.23 took: 5.71s\n",
            "Epoch 16, 40% \t train_loss: 0.29 took: 5.70s\n",
            "Epoch 16, 43% \t train_loss: 0.36 took: 5.71s\n",
            "Epoch 16, 46% \t train_loss: 0.32 took: 5.75s\n",
            "Epoch 16, 50% \t train_loss: 0.33 took: 5.79s\n",
            "Epoch 16, 53% \t train_loss: 0.35 took: 5.75s\n",
            "Epoch 16, 56% \t train_loss: 0.29 took: 5.78s\n",
            "Epoch 16, 60% \t train_loss: 0.31 took: 5.79s\n",
            "Epoch 16, 63% \t train_loss: 0.26 took: 5.80s\n",
            "Epoch 16, 66% \t train_loss: 0.29 took: 5.78s\n",
            "Epoch 16, 70% \t train_loss: 0.27 took: 5.80s\n",
            "Epoch 16, 73% \t train_loss: 0.32 took: 5.78s\n",
            "Epoch 16, 76% \t train_loss: 0.29 took: 5.80s\n",
            "Epoch 16, 80% \t train_loss: 0.29 took: 5.77s\n",
            "Epoch 16, 83% \t train_loss: 0.26 took: 5.76s\n",
            "Epoch 16, 86% \t train_loss: 0.28 took: 5.80s\n",
            "Epoch 16, 90% \t train_loss: 0.29 took: 5.84s\n",
            "Epoch 16, 93% \t train_loss: 0.31 took: 5.76s\n",
            "Epoch 16, 96% \t train_loss: 0.39 took: 5.81s\n",
            "Epoch 16, 100% \t train_loss: 0.37 took: 5.79s\n",
            "Validation loss = 0.33\n",
            "Epoch 17, 3% \t train_loss: 0.30 took: 6.62s\n",
            "Epoch 17, 6% \t train_loss: 0.38 took: 5.79s\n",
            "Epoch 17, 10% \t train_loss: 0.29 took: 5.83s\n",
            "Epoch 17, 13% \t train_loss: 0.26 took: 5.79s\n",
            "Epoch 17, 16% \t train_loss: 0.25 took: 5.93s\n",
            "Epoch 17, 20% \t train_loss: 0.36 took: 5.75s\n",
            "Epoch 17, 23% \t train_loss: 0.31 took: 5.86s\n",
            "Epoch 17, 26% \t train_loss: 0.29 took: 5.81s\n",
            "Epoch 17, 30% \t train_loss: 0.24 took: 5.79s\n",
            "Epoch 17, 33% \t train_loss: 0.30 took: 5.80s\n",
            "Epoch 17, 36% \t train_loss: 0.27 took: 5.79s\n",
            "Epoch 17, 40% \t train_loss: 0.22 took: 5.82s\n",
            "Epoch 17, 43% \t train_loss: 0.24 took: 5.79s\n",
            "Epoch 17, 46% \t train_loss: 0.35 took: 5.80s\n",
            "Epoch 17, 50% \t train_loss: 0.28 took: 5.78s\n",
            "Epoch 17, 53% \t train_loss: 0.24 took: 5.79s\n",
            "Epoch 17, 56% \t train_loss: 0.29 took: 5.83s\n",
            "Epoch 17, 60% \t train_loss: 0.24 took: 5.78s\n",
            "Epoch 17, 63% \t train_loss: 0.35 took: 5.86s\n",
            "Epoch 17, 66% \t train_loss: 0.28 took: 5.77s\n",
            "Epoch 17, 70% \t train_loss: 0.28 took: 5.81s\n",
            "Epoch 17, 73% \t train_loss: 0.33 took: 5.82s\n",
            "Epoch 17, 76% \t train_loss: 0.35 took: 5.81s\n",
            "Epoch 17, 80% \t train_loss: 0.33 took: 5.78s\n",
            "Epoch 17, 83% \t train_loss: 0.30 took: 5.79s\n",
            "Epoch 17, 86% \t train_loss: 0.36 took: 5.77s\n",
            "Epoch 17, 90% \t train_loss: 0.28 took: 5.79s\n",
            "Epoch 17, 93% \t train_loss: 0.22 took: 5.78s\n",
            "Epoch 17, 96% \t train_loss: 0.28 took: 5.83s\n",
            "Epoch 17, 100% \t train_loss: 0.30 took: 5.79s\n",
            "Validation loss = 0.29\n",
            "Epoch 18, 3% \t train_loss: 0.30 took: 6.61s\n",
            "Epoch 18, 6% \t train_loss: 0.24 took: 5.80s\n",
            "Epoch 18, 10% \t train_loss: 0.34 took: 5.81s\n",
            "Epoch 18, 13% \t train_loss: 0.35 took: 5.78s\n",
            "Epoch 18, 16% \t train_loss: 0.29 took: 5.81s\n",
            "Epoch 18, 20% \t train_loss: 0.27 took: 6.14s\n",
            "Epoch 18, 23% \t train_loss: 0.25 took: 5.82s\n",
            "Epoch 18, 26% \t train_loss: 0.21 took: 5.75s\n",
            "Epoch 18, 30% \t train_loss: 0.26 took: 5.79s\n",
            "Epoch 18, 33% \t train_loss: 0.23 took: 5.79s\n",
            "Epoch 18, 36% \t train_loss: 0.24 took: 5.82s\n",
            "Epoch 18, 40% \t train_loss: 0.34 took: 5.84s\n",
            "Epoch 18, 43% \t train_loss: 0.33 took: 5.87s\n",
            "Epoch 18, 46% \t train_loss: 0.32 took: 5.78s\n",
            "Epoch 18, 50% \t train_loss: 0.29 took: 5.83s\n",
            "Epoch 18, 53% \t train_loss: 0.30 took: 5.88s\n",
            "Epoch 18, 56% \t train_loss: 0.30 took: 5.83s\n",
            "Epoch 18, 60% \t train_loss: 0.25 took: 5.90s\n",
            "Epoch 18, 63% \t train_loss: 0.23 took: 5.83s\n",
            "Epoch 18, 66% \t train_loss: 0.25 took: 5.82s\n",
            "Epoch 18, 70% \t train_loss: 0.34 took: 5.80s\n",
            "Epoch 18, 73% \t train_loss: 0.29 took: 5.80s\n",
            "Epoch 18, 76% \t train_loss: 0.21 took: 5.82s\n",
            "Epoch 18, 80% \t train_loss: 0.22 took: 5.77s\n",
            "Epoch 18, 83% \t train_loss: 0.28 took: 5.82s\n",
            "Epoch 18, 86% \t train_loss: 0.32 took: 5.81s\n",
            "Epoch 18, 90% \t train_loss: 0.36 took: 5.83s\n",
            "Epoch 18, 93% \t train_loss: 0.30 took: 5.75s\n",
            "Epoch 18, 96% \t train_loss: 0.26 took: 5.82s\n",
            "Epoch 18, 100% \t train_loss: 0.29 took: 5.77s\n",
            "Validation loss = 0.28\n",
            "Epoch 19, 3% \t train_loss: 0.35 took: 6.59s\n",
            "Epoch 19, 6% \t train_loss: 0.30 took: 5.78s\n",
            "Epoch 19, 10% \t train_loss: 0.30 took: 5.80s\n",
            "Epoch 19, 13% \t train_loss: 0.24 took: 5.80s\n",
            "Epoch 19, 16% \t train_loss: 0.26 took: 5.76s\n",
            "Epoch 19, 20% \t train_loss: 0.24 took: 5.79s\n",
            "Epoch 19, 23% \t train_loss: 0.29 took: 5.82s\n",
            "Epoch 19, 26% \t train_loss: 0.19 took: 5.82s\n",
            "Epoch 19, 30% \t train_loss: 0.21 took: 5.80s\n",
            "Epoch 19, 33% \t train_loss: 0.21 took: 5.81s\n",
            "Epoch 19, 36% \t train_loss: 0.22 took: 5.79s\n",
            "Epoch 19, 40% \t train_loss: 0.37 took: 5.79s\n",
            "Epoch 19, 43% \t train_loss: 0.22 took: 5.82s\n",
            "Epoch 19, 46% \t train_loss: 0.27 took: 5.80s\n",
            "Epoch 19, 50% \t train_loss: 0.35 took: 5.81s\n",
            "Epoch 19, 53% \t train_loss: 0.25 took: 5.79s\n",
            "Epoch 19, 56% \t train_loss: 0.29 took: 5.83s\n",
            "Epoch 19, 60% \t train_loss: 0.24 took: 5.77s\n",
            "Epoch 19, 63% \t train_loss: 0.31 took: 5.81s\n",
            "Epoch 19, 66% \t train_loss: 0.35 took: 5.78s\n",
            "Epoch 19, 70% \t train_loss: 0.38 took: 5.79s\n",
            "Epoch 19, 73% \t train_loss: 0.33 took: 5.82s\n",
            "Epoch 19, 76% \t train_loss: 0.35 took: 5.78s\n",
            "Epoch 19, 80% \t train_loss: 0.27 took: 5.74s\n",
            "Epoch 19, 83% \t train_loss: 0.35 took: 5.78s\n",
            "Epoch 19, 86% \t train_loss: 0.40 took: 5.77s\n",
            "Epoch 19, 90% \t train_loss: 0.36 took: 5.79s\n",
            "Epoch 19, 93% \t train_loss: 0.32 took: 5.76s\n",
            "Epoch 19, 96% \t train_loss: 0.26 took: 5.74s\n",
            "Epoch 19, 100% \t train_loss: 0.32 took: 5.79s\n",
            "Validation loss = 0.30\n",
            "Epoch 20, 3% \t train_loss: 0.24 took: 6.67s\n",
            "Epoch 20, 6% \t train_loss: 0.31 took: 5.82s\n",
            "Epoch 20, 10% \t train_loss: 0.29 took: 5.84s\n",
            "Epoch 20, 13% \t train_loss: 0.26 took: 5.80s\n",
            "Epoch 20, 16% \t train_loss: 0.27 took: 5.82s\n",
            "Epoch 20, 20% \t train_loss: 0.34 took: 5.76s\n",
            "Epoch 20, 23% \t train_loss: 0.35 took: 5.85s\n",
            "Epoch 20, 26% \t train_loss: 0.32 took: 5.81s\n",
            "Epoch 20, 30% \t train_loss: 0.23 took: 5.80s\n",
            "Epoch 20, 33% \t train_loss: 0.28 took: 5.79s\n",
            "Epoch 20, 36% \t train_loss: 0.31 took: 5.83s\n",
            "Epoch 20, 40% \t train_loss: 0.41 took: 5.82s\n",
            "Epoch 20, 43% \t train_loss: 0.42 took: 5.80s\n",
            "Epoch 20, 46% \t train_loss: 0.43 took: 5.80s\n",
            "Epoch 20, 50% \t train_loss: 0.34 took: 5.79s\n",
            "Epoch 20, 53% \t train_loss: 0.27 took: 5.80s\n",
            "Epoch 20, 56% \t train_loss: 0.23 took: 5.77s\n",
            "Epoch 20, 60% \t train_loss: 0.32 took: 5.82s\n",
            "Epoch 20, 63% \t train_loss: 0.28 took: 5.78s\n",
            "Epoch 20, 66% \t train_loss: 0.25 took: 5.79s\n",
            "Epoch 20, 70% \t train_loss: 0.30 took: 5.83s\n",
            "Epoch 20, 73% \t train_loss: 0.26 took: 5.78s\n",
            "Epoch 20, 76% \t train_loss: 0.26 took: 5.80s\n",
            "Epoch 20, 80% \t train_loss: 0.25 took: 5.77s\n",
            "Epoch 20, 83% \t train_loss: 0.31 took: 5.83s\n",
            "Epoch 20, 86% \t train_loss: 0.35 took: 5.77s\n",
            "Epoch 20, 90% \t train_loss: 0.32 took: 5.80s\n",
            "Epoch 20, 93% \t train_loss: 0.32 took: 5.77s\n",
            "Epoch 20, 96% \t train_loss: 0.26 took: 5.83s\n",
            "Epoch 20, 100% \t train_loss: 0.23 took: 5.87s\n",
            "Validation loss = 0.30\n",
            "Training finished, took 5103.78s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cigZzw7XcGy-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5c93973-f77a-47cd-a2b5-5eecec1ab778"
      },
      "source": [
        "total_wrongs = 0\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=val_sampler, num_workers=2)\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "\n",
        "    #Wrap tensors in Variables\n",
        "    inputs, labels = data\n",
        "    inputs, labels = Variable(inputs.reshape((1,1,200,87))), Variable(labels)\n",
        "    #Forward pass\n",
        "    val_outputs = CNN.double()(inputs)\n",
        "    value, index = val_outputs[0].max(0)\n",
        "    total_wrongs += int(index.item() != labels[0].item())\n",
        "    print(\"\\r\", str(total_wrongs) + \"/\" + str(i + 1), \"{0:.2f}%\".format(100 * (i - total_wrongs + 1) / (i + 1)), end=\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 4572/41212 88.91%"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuiOtzV1Qh7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}