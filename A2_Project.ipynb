{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pitwegner/UTS_ML2019_Project/blob/master/A2_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReiUL--rysUT"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0dc8b9a6be6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlocation_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDATA_SOURCE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'google'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlocation_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "DATA_SOURCE = 'google'\n",
    "location_prefix = ''\n",
    "if DATA_SOURCE == 'google':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    location_prefix = '/content/drive/My Drive/'\n",
    "elif DATA_SOURCE == 'github':\n",
    "    import urllib.request   \n",
    "    dl_location = '/content/'\n",
    "    filename, headers = urllib.request.urlretrieve('https://github.com/pitwegner/UTS_ML2019_Project/archive/master.zip', filename=dl_location + 'master.zip')\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dl_location)\n",
    "    location_prefix = '/content/UTS_ML2019_Project-master/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-RL7JyBysUi"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'train/activities_train.csv' does not exist: b'train/activities_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-933c675be523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactivities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"train/activities_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Activity Labels for Segments and Nurse ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmocap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading Mocap Data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbar_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'train/activities_train.csv' does not exist: b'train/activities_train.csv'"
     ]
    }
   ],
   "source": [
    "activities = pd.read_csv(location_prefix + \"train/activities_train.csv\") # Activity Labels for Segments and Nurse ID\n",
    "mocap = pd.DataFrame()\n",
    "print(\"Reading Mocap Data\")\n",
    "i = 0\n",
    "bar_length = 50\n",
    "files = glob.glob(location_prefix + \"train/mocap/segment*.csv\")\n",
    "for mf in files:\n",
    "    i += 1\n",
    "    progress = math.ceil(bar_length * i / len(files))\n",
    "    print(\"\\r\", \"[\" + \"=\" * progress + \" \" * (bar_length - progress) + \"] \" + \"{0:.2f}\".format(100 * i / len(files)) + '%', end=\"\")\n",
    "    mocap = mocap.append(pd.read_csv(mf).ffill().bfill().fillna(0))\n",
    "mocap = mocap.reset_index().drop(columns=['index','time_elapsed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_nu = np.array(mocap)\n",
    "mocap_nu_id = mocap_nu[:,-1]\n",
    "mocap_nu_ft = mocap_nu[:,:-1]\n",
    "mocap_nu_ft = mocap_nu_ft.T\n",
    "mocap_nu_ft = mocap_nu_ft.reshape(29,3,1577775)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_point = int((len(mocap.columns)-1)/3)              #29 points\n",
    "total_dis = int(num_point*(num_point-1)/2)             #406 distances\n",
    "num_sample = len(mocap)                                #1577775 samples\n",
    "mocap_dis = np.zeros((total_dis,num_sample))           #shape (406,1577775)\n",
    "print('num_point:'+str(num_point),'total_dis:'+str(total_dis),'num_sample:'+str(num_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dis(a1,a2):\n",
    "  dis = np.sqrt(np.sum((a1-a2)**2,axis=0,keepdims=True))\n",
    "  return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dis_table():\n",
    "    m = 0\n",
    "    for l in range(0,num_point):\n",
    "        print('i:'+str(l))                     #shape (3,1577775)\n",
    "        a1 = mocap_nu_ft[l, :, :]\n",
    "        print(a1.shape)\n",
    "        for k in range(l+1,num_point):\n",
    "            a2 = mocap_nu_ft[k, :, :]              #shape(3.1577775)\n",
    "            dis = calculate_dis(a1, a2)           #shape(1,1577775)\n",
    "            mocap_dis[m, :] = dis\n",
    "            m+=1\n",
    "    return mocap_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mocap_dis = create_dis_table()        #shape (406,1577775)\n",
    "mocap_distance = np.zeros((num_sample, total_dis+1))   #shape (1577775, 407)\n",
    "mocap_dist = mocap_dis.T                               #shape( 1577775,406)\n",
    "mocap_distance[:, :-1] = mocap_dist\n",
    "mocap_distance[:, -1] = mocap_nu_id\n",
    "mocap_distance = pd.DataFrame(mocap_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eM-losmrzvIu"
   },
   "outputs": [],
   "source": [
    "mocap_normalized = (mocap-mocap.min())/(mocap.max()-mocap.min())\n",
    "mocap_normalized.segment_id = mocap.segment_id\n",
    "mocap = mocap_normalized\n",
    "activity_arr = activities.activity_id.unique()\n",
    "activity_arr.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8N1M0M0W2_K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  \n",
    "    def __init__(self, train, labels):\n",
    "        self.labels = labels\n",
    "        self.data = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index].drop(columns=['segment_id']).values\n",
    "        sid = self.data[index].segment_id.unique()[0]\n",
    "        labels = self.labels[self.labels.segment_id == sid]\n",
    "        aid = labels.activity_id.values[0]\n",
    "        y = np.array([activity_arr.tolist().index(aid), sid])\n",
    "\n",
    "        return X, y\n",
    "\n",
    "dataset = Dataset(mocap, activities)\n",
    "window_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in activities.subject.unique():\n",
    "    act = activities[activities.subject == a]\n",
    "    print(a, \":\", [(i, len(act[act.activity_id == i].segment_id)) for i in np.sort(act.activity_id.unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-qq2hNXysVJ"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.kernel_size = 3\n",
    "        self.stride = 1\n",
    "        self.padding = 1\n",
    "        self.output_channels = 24\n",
    "        self.hidden_parameters = 64\n",
    "        \n",
    "        self.output_x = int((window_length - self.kernel_size + 2 * self.padding) / self.stride) + 1\n",
    "        self.output_y = int((dataset[0:1][0].shape[1] - self.kernel_size + 2 * self.padding) / self.stride) + 1\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, self.output_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = torch.nn.Linear(self.output_channels * int(self.output_x / 2) * int(self.output_y / 2), self.hidden_parameters)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_parameters, len(activities.activity_id.unique()))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, self.output_channels * int(self.output_x / 2) * int(self.output_y / 2))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9yWA08P7wFg"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import Sampler    \n",
    "\n",
    "class RandomWindowSampler(Sampler):\n",
    "  \n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (slice(self.indices[i], self.indices[i] + window_length) for i in torch.randperm(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdZrZemGHkB_"
   },
   "outputs": [],
   "source": [
    "PERSON_SPLIT = True\n",
    "TEST_PERSON = 2\n",
    "VAL_PERSON = 4\n",
    "if PERSON_SPLIT:\n",
    "    indices = {}\n",
    "    for sid in dataset.data.segment_id.unique():\n",
    "        i = list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
    "        p = activities[activities.segment_id == sid].subject.item()\n",
    "        if p not in indices:\n",
    "            indices[p] = []\n",
    "        indices[p] += i\n",
    "    test_indices = indices.pop(TEST_PERSON)\n",
    "    val_indices = indices.pop(VAL_PERSON)\n",
    "    train_indices = [item for sublist in indices.values() for item in sublist]\n",
    "else:\n",
    "    train_indices, val_indices, test_indices = ([],[],[])\n",
    "    segments = dataset.data.segment_id.unique()\n",
    "    split = int(np.floor(0.15 * len(segments)))\n",
    "    np.random.shuffle(segments)\n",
    "    train, val, test = segments[split+split:], segments[split:split+split], segments[:split]\n",
    "    for sid in train:\n",
    "        train_indices += list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
    "    for sid in val:\n",
    "        val_indices += list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
    "    for sid in test:\n",
    "        test_indices += list(dataset.data[dataset.data.segment_id == sid].index[0:-window_length:50])\n",
    "\n",
    "train_sampler = RandomWindowSampler(train_indices)\n",
    "val_sampler = RandomWindowSampler(val_indices)\n",
    "test_sampler = RandomWindowSampler(test_indices)\n",
    "\n",
    "def get_train_loader(batch_size):\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "  \n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=128, sampler=val_sampler, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=4, sampler=test_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZvSGvD2I5JT"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "  \n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size =\", batch_size)\n",
    "    print(\"epochs =\", n_epochs)\n",
    "    print(\"learning_rate =\", learning_rate)\n",
    "    print(\"=\" * 27)\n",
    "    \n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    min_val_loss = math.inf\n",
    "    worse_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        total_train_loss = 0.0\n",
    "        print_every = 10\n",
    "        start_time = time.time()\n",
    "        \n",
    "        worse_counter += 1\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "            if inputs.shape != (32,200,87):\n",
    "                # TODO: Handle leftover batches (<32)\n",
    "                continue\n",
    "            inputs, labels = Variable(inputs.reshape((32,1,200,87))), Variable(labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels[:,0])\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss_size.data.item()\n",
    "            total_train_loss += loss_size.data.item()\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(epoch+1, int(100 * (i+1) / len(train_loader)), running_loss / print_every, time.time() - start_time))\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "        total_val_loss = 0.0\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            inputs, labels = data\n",
    "            if inputs.shape != (128,200,87):\n",
    "                break\n",
    "            inputs, labels = Variable(inputs.reshape((128,1,200,87))), Variable(labels)\n",
    "\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels[:,0])\n",
    "            total_val_loss += val_loss_size.data.item()\n",
    "            \n",
    "        loss_avg = total_val_loss / len(val_loader)\n",
    "        print(\"Validation loss = {:.2f}\".format(loss_avg))\n",
    "        if loss_avg < min_val_loss:\n",
    "            min_val_loss = loss_avg\n",
    "            best_model = net\n",
    "            worse_counter = 0\n",
    "        \n",
    "        val_losses.append(loss_avg)\n",
    "        train_losses.append(total_train_loss / len(train_loader))\n",
    "        if worse_counter >= 10:\n",
    "            break\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCwSnqOBJerf"
   },
   "outputs": [],
   "source": [
    "CNN = SimpleCNN()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_model = CNN\n",
    "trainNet(CNN.double(), batch_size=32, n_epochs=150, learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuiOtzV1Qh7R"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(train_losses)), y=train_losses, mode='lines', name='train_loss'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(val_losses)), y=val_losses, mode='lines', name='val_loss'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cigZzw7XcGy-"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "confusion_matrix = np.zeros((6,6))\n",
    "votes = {}\n",
    "print(\"Starting Test Run\")\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    if inputs.shape != (4,200,87):\n",
    "        # TODO: Handle leftover batches (<4)\n",
    "        break\n",
    "    inputs, labels = Variable(inputs.reshape((4,1,200,87))), Variable(labels)\n",
    "    val_outputs = CNN.double()(inputs)\n",
    "    preds = val_outputs.argmax(1)\n",
    "    for s in range(len(labels)):\n",
    "        if labels[s,1].item() not in votes:\n",
    "            votes[labels[s,1].item()] = []\n",
    "        votes[labels[s,1].item()].append(preds[s].item())\n",
    "    confusion_matrix[preds, labels[:,0]] += 1\n",
    "    print(\"\\r\", \"{0:.2f}%\".format(100 * i / len(test_loader)), end=\"\")\n",
    "for sid in votes:\n",
    "    label = dataset.labels[dataset.labels.segment_id == sid].activity_id.values[0]\n",
    "    votes[sid] = [stats.mode(votes[sid])[0][0], label]\n",
    "print(\"\\n\")\n",
    "print(votes)\n",
    "print(confusion_matrix)\n",
    "print(\"{0:.2f}%\".format(100 * np.trace(confusion_matrix)/np.sum(confusion_matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "confusion_matrix = np.zeros((6,6))\n",
    "votes = {}\n",
    "print(\"Starting Test Run\")\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    if inputs.shape != (4,200,87):\n",
    "        # TODO: Handle leftover batches (<4)\n",
    "        break\n",
    "    inputs, labels = Variable(inputs.reshape((4,1,200,87))), Variable(labels)\n",
    "    val_outputs = best_model.double()(inputs)\n",
    "    preds = val_outputs.argmax(1)\n",
    "    for s in range(len(labels)):\n",
    "        if labels[s,1].item() not in votes:\n",
    "            votes[labels[s,1].item()] = []\n",
    "        votes[labels[s,1].item()].append(preds[s].item())\n",
    "    confusion_matrix[preds, labels[:,0]] += 1\n",
    "    print(\"\\r\", \"{0:.2f}%\".format(100 * i / len(test_loader)), end=\"\")\n",
    "for sid in votes:\n",
    "    label = dataset.labels[dataset.labels.segment_id == sid].activity_id.values[0]\n",
    "    votes[sid] = [stats.mode(votes[sid])[0][0], label]\n",
    "print(\"\\n\")\n",
    "print(votes)\n",
    "print(confusion_matrix)\n",
    "print(\"{0:.2f}%\".format(100 * np.trace(confusion_matrix)/np.sum(confusion_matrix)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "A2_Project.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
